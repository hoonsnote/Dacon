{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1c8ENUCQXO9AzRlyzlWlQ1NPmHssM29s2","authorship_tag":"ABX9TyMw11P5aq4D9QvwBIFTloLr"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3b982d839dfe40ecaf97254111b7f23c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_57d99b51020a46188f890c15966765ed","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4eda7db6afa34506888b6ed45d39c1ce","IPY_MODEL_47f4be8b0dca4e60a5df40cd7d09d13d"]}},"57d99b51020a46188f890c15966765ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4eda7db6afa34506888b6ed45d39c1ce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c2027aa1b794407fb46c612cef296eb7","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":52128,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":52128,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a3ddd8ae46fa4baa92ae9a5c986b87d0"}},"47f4be8b0dca4e60a5df40cd7d09d13d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f9ca101bf34c40998f509af92f8f77f7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 52128/52128 [36:15&lt;00:00, 23.97it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_498116b06b034481a6079dfe39870d38"}},"c2027aa1b794407fb46c612cef296eb7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a3ddd8ae46fa4baa92ae9a5c986b87d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f9ca101bf34c40998f509af92f8f77f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"498116b06b034481a6079dfe39870d38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"95e5031f933f45388575fb84ee99d88a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_309db83d470c4ccd85b6d6127227a62d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_967b3ce2402744488ac202beec211f48","IPY_MODEL_47674455fd8f49d881963d71f57b8246"]}},"309db83d470c4ccd85b6d6127227a62d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"967b3ce2402744488ac202beec211f48":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0052c487f0ef421a812b9f8a9151311f","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":9,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_142ec8905e8a4830a97d108d53192d61"}},"47674455fd8f49d881963d71f57b8246":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6319ea45d3b3473dbd76512f18833b06","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9/9 [42:06&lt;00:00, 280.67s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_954c9c5609ea49fea9480a3a7fd55c1b"}},"0052c487f0ef421a812b9f8a9151311f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"142ec8905e8a4830a97d108d53192d61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6319ea45d3b3473dbd76512f18833b06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"954c9c5609ea49fea9480a3a7fd55c1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"s1JeuY-wmmBa"},"source":["import warnings\n","warnings.filterwarnings(action='ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UE5KCwx9m0LJ"},"source":["import pandas as pd\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, RobustScaler, MinMaxScaler\n","import seaborn as sns\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.metrics import accuracy_score, recall_score, roc_curve, precision_score, f1_score, auc\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","import tensorflow as tf\n","import keras\n","import keras.backend as K\n","from keras import metrics\n","from tensorflow.keras.models import Sequential, Model,load_model\n","from tensorflow.keras.layers import Permute,multiply,Add,Multiply,BatchNormalization,Dropout, Conv1D, Input, Flatten, Bidirectional, MaxPooling1D, Activation, Flatten, Dense, Dropout, BatchNormalization, LSTM, TimeDistributed, SpatialDropout1D, GaussianNoise\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.layers.merge import concatenate\n","from sklearn.metrics import mean_absolute_error"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c3v8EUdvqvT5"},"source":["# 데이터 불러오기"]},{"cell_type":"code","metadata":{"id":"CPFoUF86nEWK"},"source":["train=pd.read_csv('/content/drive/MyDrive/dacon/solar/train/train.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YU4pRDeKlIRE"},"source":["for i in range(0,81):\n","    test = \"test_%d = pd.read_csv('/content/drive/MyDrive/dacon/solar/test/%d.csv')\"%(i,i)\n","    exec(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ud0pvwE-CnpQ"},"source":["# DHI, DNI, T 변수만 활용\n","X_train=train.drop(['Day','Hour','Minute','WS','RH'],axis=1)\n","y_train=train[['TARGET']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OGeqbDFzRijM"},"source":["# 데이터 전처리"]},{"cell_type":"code","metadata":{"id":"fpQVr9DTVJy0"},"source":["# Scaling \n","scaling=RobustScaler()\n","X_train[X_train.columns]=scaling.fit_transform(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qX2wyUB2b1G"},"source":["# X(7일), y1(1일 뒤 예측), y2(2일 뒤 예측) 데이터 분할\n","def multivariate_split(dataset, target, start_index, end_index, history_size,\n","                      target_size, step, single_step=False):\n","  data = []\n","  labels1 = []\n","  labels2 = []\n","\n","  start_index = start_index + history_size\n","  if end_index is None:\n","    end_index = len(dataset) - target_size\n","\n","  for i in tqdm(range(start_index, end_index)):\n","    indices = range(i-history_size, i, step)\n","    data.append(dataset.loc[indices])\n","\n","    if single_step:\n","      labels.append(target.loc[i+target_size-1,:])\n","    else:\n","      labels1.append(target.loc[i:i+target_size-1-48,:])\n","      labels2.append(target.loc[i+48:i+target_size-1,:])\n","\n","  return np.array(data), np.array(labels1), np.array(labels2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["3b982d839dfe40ecaf97254111b7f23c","57d99b51020a46188f890c15966765ed","4eda7db6afa34506888b6ed45d39c1ce","47f4be8b0dca4e60a5df40cd7d09d13d","c2027aa1b794407fb46c612cef296eb7","a3ddd8ae46fa4baa92ae9a5c986b87d0","f9ca101bf34c40998f509af92f8f77f7","498116b06b034481a6079dfe39870d38"]},"id":"QtrJjpzhtvvM","executionInfo":{"status":"ok","timestamp":1617786917641,"user_tz":-540,"elapsed":33400,"user":{"displayName":"신동훈","photoUrl":"","userId":"05309578636363571738"}},"outputId":"c58b10f0-a49a-4582-ce96-4f9320cf912f"},"source":["history_size=336\n","target_size=96\n","step=1\n","\n","X_train1, y_train1, y_train2 = multivariate_split(X_train,y_train,\n","                                      0, len(X_train)-96, \n","                                      history_size, target_size, step)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b982d839dfe40ecaf97254111b7f23c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=52128.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0xq6OTSdJjAX"},"source":["# 연속된 Sequence Dataset Shuffle\n","s = np.arange(X_train1.shape[0])\n","np.random.shuffle(s)\n","\n","X_train_1 = X_train1[s]\n","y_train_1 = y_train1[s]\n","y_train_2 = y_train2[s]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ROpWxz5gx5Yb","executionInfo":{"status":"ok","timestamp":1617786917643,"user_tz":-540,"elapsed":33395,"user":{"displayName":"신동훈","photoUrl":"","userId":"05309578636363571738"}},"outputId":"ebb8edee-7fb6-40bc-b4cb-441dba458693"},"source":["print(X_train_1.shape,y_train_1.shape,y_train_2.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(52128, 336, 4) (52128, 48, 1) (52128, 48, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EWegzF-2bApF"},"source":["# CNN Modeling"]},{"cell_type":"code","metadata":{"id":"MVIrQRoH0WwK"},"source":["# custom loss function\n","def quantile_loss(q,y,f):\n","    e = (y-f)\n","    return K.mean(K.maximum(q*e, (q-1)*e), axis=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pOzfmIc_T_n3"},"source":["def solar_model():\n","  num=64\n","  strides_size=1\n","  model_input = Input(shape=(336,4))\n","  model = Conv1D(num,3,padding='same',activation='relu')(model_input)\n","  model = Conv1D(num,7,padding='same',activation='relu',dilation_rate=48)(model)\n","  model = Conv1D(num,2,padding='valid',activation='relu',dilation_rate=48)(model)\n","  model = Conv1D(num,2,padding='valid',activation='relu',dilation_rate=48)(model)\n","  model = Conv1D(num,2,padding='valid',activation='relu',dilation_rate=48)(model)\n","  model = Conv1D(num,2,padding='valid',activation='relu',dilation_rate=48)(model)\n","  model = Conv1D(num/2,2,padding='valid',activation='relu',dilation_rate=48)(model)\n","  model = Conv1D(num/4,2,padding='valid',activation='relu',dilation_rate=48)(model)\n","  model = Conv1D(1,1,activation='relu',dilation_rate=48)(model)\n","  model = Flatten()(model)\n","  model = Model(inputs=model_input, outputs = model)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"72xtB07L7MI5","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["95e5031f933f45388575fb84ee99d88a","309db83d470c4ccd85b6d6127227a62d","967b3ce2402744488ac202beec211f48","47674455fd8f49d881963d71f57b8246","0052c487f0ef421a812b9f8a9151311f","142ec8905e8a4830a97d108d53192d61","6319ea45d3b3473dbd76512f18833b06","954c9c5609ea49fea9480a3a7fd55c1b"]},"executionInfo":{"status":"ok","timestamp":1617789632862,"user_tz":-540,"elapsed":2527108,"user":{"displayName":"신동훈","photoUrl":"","userId":"05309578636363571738"}},"outputId":"87551624-6d55-4b15-ac13-39db09dc8036"},"source":["qs = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n","prediction=[]\n","\n","for q in tqdm(qs):\n","  print('='*20 + ' ' + str(q) + 'day1' +' ' + '='*20)\n","  model_1 = solar_model()\n","  adam=keras.optimizers.Adam(lr=0.0001)\n","  model_1.compile(loss=lambda y,f: quantile_loss(q,y,f), optimizer=adam)\n","  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n","  mc = ModelCheckpoint('best_model_1.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n","  history=model_1.fit(X_train_1, y_train_1, epochs=30, callbacks=[es,mc], validation_split=0.2, batch_size=128)\n","  model_1.load_weights('/content/best_model_1.h5')\n","  \n","  print('='*20 + ' ' + str(q) + 'day2' + ' ' + '='*20)\n","  model_2 = solar_model()\n","  adam=keras.optimizers.Adam(lr=0.0001)\n","  model_2.compile(loss=lambda y,f: quantile_loss(q,y,f), optimizer=adam)\n","  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n","  mc = ModelCheckpoint('best_model_2.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n","  history=model_2.fit(X_train_1, y_train_2, epochs=30, callbacks=[es,mc], validation_split=0.2, batch_size=128)\n","  model_2.load_weights('/content/best_model_2.h5')\n","\n","  pred_total = []\n","  for i in range(0,81):\n","    tmp = pd.read_csv(f'/content/drive/MyDrive/dacon/solar/test/{i}.csv')\n","    tmp = tmp.drop(['Day','Hour','Minute','WS','RH'],axis=1)\n","    tmp = scaling.transform(tmp)\n","    tmp=np.array(tmp).reshape(1,336,4)\n","    pred = []\n","    pred.extend(model_1.predict(tmp))\n","    pred.extend(model_2.predict(tmp))\n","    pred_total.append(pred)\n","  prediction.append(np.array(pred_total).reshape(96*81))\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95e5031f933f45388575fb84ee99d88a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["==================== 0.1day1 ====================\n","Epoch 1/30\n","326/326 [==============================] - 6s 15ms/step - loss: 1.5993 - val_loss: 1.4115\n","\n","Epoch 00001: val_loss improved from inf to 1.41153, saving model to best_model_1.h5\n","Epoch 2/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.4025 - val_loss: 1.3845\n","\n","Epoch 00002: val_loss improved from 1.41153 to 1.38446, saving model to best_model_1.h5\n","Epoch 3/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3776 - val_loss: 1.3618\n","\n","Epoch 00003: val_loss improved from 1.38446 to 1.36184, saving model to best_model_1.h5\n","Epoch 4/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3601 - val_loss: 1.3408\n","\n","Epoch 00004: val_loss improved from 1.36184 to 1.34079, saving model to best_model_1.h5\n","Epoch 5/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3328 - val_loss: 1.3162\n","\n","Epoch 00005: val_loss improved from 1.34079 to 1.31618, saving model to best_model_1.h5\n","Epoch 6/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.3121 - val_loss: 1.3039\n","\n","Epoch 00006: val_loss improved from 1.31618 to 1.30389, saving model to best_model_1.h5\n","Epoch 7/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2864 - val_loss: 1.2681\n","\n","Epoch 00007: val_loss improved from 1.30389 to 1.26806, saving model to best_model_1.h5\n","Epoch 8/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.2545 - val_loss: 1.2305\n","\n","Epoch 00008: val_loss improved from 1.26806 to 1.23054, saving model to best_model_1.h5\n","Epoch 9/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2238 - val_loss: 1.2000\n","\n","Epoch 00009: val_loss improved from 1.23054 to 1.20001, saving model to best_model_1.h5\n","Epoch 10/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.1957 - val_loss: 1.1758\n","\n","Epoch 00010: val_loss improved from 1.20001 to 1.17585, saving model to best_model_1.h5\n","Epoch 11/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.1657 - val_loss: 1.1436\n","\n","Epoch 00011: val_loss improved from 1.17585 to 1.14360, saving model to best_model_1.h5\n","Epoch 12/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.1280 - val_loss: 1.1337\n","\n","Epoch 00012: val_loss improved from 1.14360 to 1.13369, saving model to best_model_1.h5\n","Epoch 13/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.1106 - val_loss: 1.1007\n","\n","Epoch 00013: val_loss improved from 1.13369 to 1.10066, saving model to best_model_1.h5\n","Epoch 14/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.0707 - val_loss: 1.0777\n","\n","Epoch 00014: val_loss improved from 1.10066 to 1.07767, saving model to best_model_1.h5\n","Epoch 15/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.0525 - val_loss: 1.0324\n","\n","Epoch 00015: val_loss improved from 1.07767 to 1.03236, saving model to best_model_1.h5\n","Epoch 16/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.0210 - val_loss: 1.0102\n","\n","Epoch 00016: val_loss improved from 1.03236 to 1.01023, saving model to best_model_1.h5\n","Epoch 17/30\n","326/326 [==============================] - 4s 13ms/step - loss: 0.9970 - val_loss: 0.9966\n","\n","Epoch 00017: val_loss improved from 1.01023 to 0.99664, saving model to best_model_1.h5\n","Epoch 18/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.9750 - val_loss: 0.9826\n","\n","Epoch 00018: val_loss improved from 0.99664 to 0.98265, saving model to best_model_1.h5\n","Epoch 19/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.9510 - val_loss: 0.9476\n","\n","Epoch 00019: val_loss improved from 0.98265 to 0.94762, saving model to best_model_1.h5\n","Epoch 20/30\n","326/326 [==============================] - 4s 13ms/step - loss: 0.9286 - val_loss: 0.9489\n","\n","Epoch 00020: val_loss did not improve from 0.94762\n","Epoch 21/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.9154 - val_loss: 0.8977\n","\n","Epoch 00021: val_loss improved from 0.94762 to 0.89769, saving model to best_model_1.h5\n","Epoch 22/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.8912 - val_loss: 0.8869\n","\n","Epoch 00022: val_loss improved from 0.89769 to 0.88695, saving model to best_model_1.h5\n","Epoch 23/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.8666 - val_loss: 0.8670\n","\n","Epoch 00023: val_loss improved from 0.88695 to 0.86696, saving model to best_model_1.h5\n","Epoch 24/30\n","326/326 [==============================] - 4s 13ms/step - loss: 0.8562 - val_loss: 0.8455\n","\n","Epoch 00024: val_loss improved from 0.86696 to 0.84546, saving model to best_model_1.h5\n","Epoch 25/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.8416 - val_loss: 0.8374\n","\n","Epoch 00025: val_loss improved from 0.84546 to 0.83743, saving model to best_model_1.h5\n","Epoch 26/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.8255 - val_loss: 0.8297\n","\n","Epoch 00026: val_loss improved from 0.83743 to 0.82966, saving model to best_model_1.h5\n","Epoch 27/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.8138 - val_loss: 0.8179\n","\n","Epoch 00027: val_loss improved from 0.82966 to 0.81788, saving model to best_model_1.h5\n","Epoch 28/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.8142 - val_loss: 0.7983\n","\n","Epoch 00028: val_loss improved from 0.81788 to 0.79833, saving model to best_model_1.h5\n","Epoch 29/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.7808 - val_loss: 0.7896\n","\n","Epoch 00029: val_loss improved from 0.79833 to 0.78963, saving model to best_model_1.h5\n","Epoch 30/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.7769 - val_loss: 0.7706\n","\n","Epoch 00030: val_loss improved from 0.78963 to 0.77056, saving model to best_model_1.h5\n","==================== 0.1day2 ====================\n","Epoch 1/30\n","326/326 [==============================] - 6s 15ms/step - loss: 1.6174 - val_loss: 1.4477\n","\n","Epoch 00001: val_loss improved from inf to 1.44768, saving model to best_model_2.h5\n","Epoch 2/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.4289 - val_loss: 1.4276\n","\n","Epoch 00002: val_loss improved from 1.44768 to 1.42756, saving model to best_model_2.h5\n","Epoch 3/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.4156 - val_loss: 1.4111\n","\n","Epoch 00003: val_loss improved from 1.42756 to 1.41109, saving model to best_model_2.h5\n","Epoch 4/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.3904 - val_loss: 1.3829\n","\n","Epoch 00004: val_loss improved from 1.41109 to 1.38290, saving model to best_model_2.h5\n","Epoch 5/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.3714 - val_loss: 1.3630\n","\n","Epoch 00005: val_loss improved from 1.38290 to 1.36301, saving model to best_model_2.h5\n","Epoch 6/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.3520 - val_loss: 1.3520\n","\n","Epoch 00006: val_loss improved from 1.36301 to 1.35198, saving model to best_model_2.h5\n","Epoch 7/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.3216 - val_loss: 1.3135\n","\n","Epoch 00007: val_loss improved from 1.35198 to 1.31354, saving model to best_model_2.h5\n","Epoch 8/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.3071 - val_loss: 1.3165\n","\n","Epoch 00008: val_loss did not improve from 1.31354\n","Epoch 9/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.2788 - val_loss: 1.2588\n","\n","Epoch 00009: val_loss improved from 1.31354 to 1.25875, saving model to best_model_2.h5\n","Epoch 10/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.2530 - val_loss: 1.2570\n","\n","Epoch 00010: val_loss improved from 1.25875 to 1.25699, saving model to best_model_2.h5\n","Epoch 11/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2196 - val_loss: 1.2109\n","\n","Epoch 00011: val_loss improved from 1.25699 to 1.21089, saving model to best_model_2.h5\n","Epoch 12/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.1957 - val_loss: 1.2274\n","\n","Epoch 00012: val_loss did not improve from 1.21089\n","Epoch 13/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.1641 - val_loss: 1.1601\n","\n","Epoch 00013: val_loss improved from 1.21089 to 1.16007, saving model to best_model_2.h5\n","Epoch 14/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.1426 - val_loss: 1.1271\n","\n","Epoch 00014: val_loss improved from 1.16007 to 1.12714, saving model to best_model_2.h5\n","Epoch 15/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.1122 - val_loss: 1.1156\n","\n","Epoch 00015: val_loss improved from 1.12714 to 1.11562, saving model to best_model_2.h5\n","Epoch 16/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.0884 - val_loss: 1.0790\n","\n","Epoch 00016: val_loss improved from 1.11562 to 1.07901, saving model to best_model_2.h5\n","Epoch 17/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.0697 - val_loss: 1.0847\n","\n","Epoch 00017: val_loss did not improve from 1.07901\n","Epoch 18/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.0538 - val_loss: 1.0391\n","\n","Epoch 00018: val_loss improved from 1.07901 to 1.03911, saving model to best_model_2.h5\n","Epoch 19/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.0270 - val_loss: 1.0276\n","\n","Epoch 00019: val_loss improved from 1.03911 to 1.02761, saving model to best_model_2.h5\n","Epoch 20/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.0085 - val_loss: 1.0002\n","\n","Epoch 00020: val_loss improved from 1.02761 to 1.00025, saving model to best_model_2.h5\n","Epoch 21/30\n","326/326 [==============================] - 5s 15ms/step - loss: 0.9966 - val_loss: 1.0024\n","\n","Epoch 00021: val_loss did not improve from 1.00025\n","Epoch 22/30\n","326/326 [==============================] - 5s 15ms/step - loss: 0.9838 - val_loss: 0.9758\n","\n","Epoch 00022: val_loss improved from 1.00025 to 0.97583, saving model to best_model_2.h5\n","Epoch 23/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.9640 - val_loss: 0.9509\n","\n","Epoch 00023: val_loss improved from 0.97583 to 0.95086, saving model to best_model_2.h5\n","Epoch 24/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.9472 - val_loss: 0.9383\n","\n","Epoch 00024: val_loss improved from 0.95086 to 0.93834, saving model to best_model_2.h5\n","Epoch 25/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.9378 - val_loss: 0.9256\n","\n","Epoch 00025: val_loss improved from 0.93834 to 0.92557, saving model to best_model_2.h5\n","Epoch 26/30\n","326/326 [==============================] - 5s 15ms/step - loss: 0.9194 - val_loss: 0.9238\n","\n","Epoch 00026: val_loss improved from 0.92557 to 0.92381, saving model to best_model_2.h5\n","Epoch 27/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.9075 - val_loss: 0.9022\n","\n","Epoch 00027: val_loss improved from 0.92381 to 0.90224, saving model to best_model_2.h5\n","Epoch 28/30\n","326/326 [==============================] - 4s 13ms/step - loss: 0.8942 - val_loss: 0.8981\n","\n","Epoch 00028: val_loss improved from 0.90224 to 0.89811, saving model to best_model_2.h5\n","Epoch 29/30\n","326/326 [==============================] - 5s 15ms/step - loss: 0.8759 - val_loss: 0.8709\n","\n","Epoch 00029: val_loss improved from 0.89811 to 0.87087, saving model to best_model_2.h5\n","Epoch 30/30\n","326/326 [==============================] - 5s 15ms/step - loss: 0.8667 - val_loss: 0.8995\n","\n","Epoch 00030: val_loss did not improve from 0.87087\n","==================== 0.2day1 ====================\n","Epoch 1/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.9930 - val_loss: 2.2896\n","\n","Epoch 00001: val_loss improved from inf to 2.28956, saving model to best_model_1.h5\n","Epoch 2/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.2826 - val_loss: 2.2534\n","\n","Epoch 00002: val_loss improved from 2.28956 to 2.25336, saving model to best_model_1.h5\n","Epoch 3/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.2444 - val_loss: 2.2252\n","\n","Epoch 00003: val_loss improved from 2.25336 to 2.22518, saving model to best_model_1.h5\n","Epoch 4/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.2089 - val_loss: 2.2302\n","\n","Epoch 00004: val_loss did not improve from 2.22518\n","Epoch 5/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.2050 - val_loss: 2.1996\n","\n","Epoch 00005: val_loss improved from 2.22518 to 2.19958, saving model to best_model_1.h5\n","Epoch 6/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1791 - val_loss: 2.1728\n","\n","Epoch 00006: val_loss improved from 2.19958 to 2.17283, saving model to best_model_1.h5\n","Epoch 7/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.1695 - val_loss: 2.1539\n","\n","Epoch 00007: val_loss improved from 2.17283 to 2.15386, saving model to best_model_1.h5\n","Epoch 8/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.1537 - val_loss: 2.1356\n","\n","Epoch 00008: val_loss improved from 2.15386 to 2.13558, saving model to best_model_1.h5\n","Epoch 9/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1268 - val_loss: 2.1409\n","\n","Epoch 00009: val_loss did not improve from 2.13558\n","Epoch 10/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1053 - val_loss: 2.0953\n","\n","Epoch 00010: val_loss improved from 2.13558 to 2.09535, saving model to best_model_1.h5\n","Epoch 11/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.0805 - val_loss: 2.0928\n","\n","Epoch 00011: val_loss improved from 2.09535 to 2.09282, saving model to best_model_1.h5\n","Epoch 12/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.0662 - val_loss: 2.0620\n","\n","Epoch 00012: val_loss improved from 2.09282 to 2.06198, saving model to best_model_1.h5\n","Epoch 13/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.0385 - val_loss: 2.0233\n","\n","Epoch 00013: val_loss improved from 2.06198 to 2.02325, saving model to best_model_1.h5\n","Epoch 14/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.0091 - val_loss: 2.0052\n","\n","Epoch 00014: val_loss improved from 2.02325 to 2.00519, saving model to best_model_1.h5\n","Epoch 15/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.9872 - val_loss: 1.9703\n","\n","Epoch 00015: val_loss improved from 2.00519 to 1.97029, saving model to best_model_1.h5\n","Epoch 16/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.9576 - val_loss: 1.9420\n","\n","Epoch 00016: val_loss improved from 1.97029 to 1.94198, saving model to best_model_1.h5\n","Epoch 17/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.9211 - val_loss: 1.9123\n","\n","Epoch 00017: val_loss improved from 1.94198 to 1.91234, saving model to best_model_1.h5\n","Epoch 18/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.8956 - val_loss: 1.8830\n","\n","Epoch 00018: val_loss improved from 1.91234 to 1.88299, saving model to best_model_1.h5\n","Epoch 19/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.8615 - val_loss: 1.8501\n","\n","Epoch 00019: val_loss improved from 1.88299 to 1.85008, saving model to best_model_1.h5\n","Epoch 20/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.8247 - val_loss: 1.8080\n","\n","Epoch 00020: val_loss improved from 1.85008 to 1.80803, saving model to best_model_1.h5\n","Epoch 21/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7920 - val_loss: 1.7835\n","\n","Epoch 00021: val_loss improved from 1.80803 to 1.78354, saving model to best_model_1.h5\n","Epoch 22/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7481 - val_loss: 1.7498\n","\n","Epoch 00022: val_loss improved from 1.78354 to 1.74983, saving model to best_model_1.h5\n","Epoch 23/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7365 - val_loss: 1.7277\n","\n","Epoch 00023: val_loss improved from 1.74983 to 1.72771, saving model to best_model_1.h5\n","Epoch 24/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7011 - val_loss: 1.6861\n","\n","Epoch 00024: val_loss improved from 1.72771 to 1.68613, saving model to best_model_1.h5\n","Epoch 25/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.6701 - val_loss: 1.6915\n","\n","Epoch 00025: val_loss did not improve from 1.68613\n","Epoch 26/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.6362 - val_loss: 1.6739\n","\n","Epoch 00026: val_loss improved from 1.68613 to 1.67386, saving model to best_model_1.h5\n","Epoch 27/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.6142 - val_loss: 1.5988\n","\n","Epoch 00027: val_loss improved from 1.67386 to 1.59877, saving model to best_model_1.h5\n","Epoch 28/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.5790 - val_loss: 1.5835\n","\n","Epoch 00028: val_loss improved from 1.59877 to 1.58350, saving model to best_model_1.h5\n","Epoch 29/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.5552 - val_loss: 1.5649\n","\n","Epoch 00029: val_loss improved from 1.58350 to 1.56489, saving model to best_model_1.h5\n","Epoch 30/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.5377 - val_loss: 1.5196\n","\n","Epoch 00030: val_loss improved from 1.56489 to 1.51959, saving model to best_model_1.h5\n","==================== 0.2day2 ====================\n","Epoch 1/30\n","326/326 [==============================] - 6s 15ms/step - loss: 3.0566 - val_loss: 2.3932\n","\n","Epoch 00001: val_loss improved from inf to 2.39325, saving model to best_model_2.h5\n","Epoch 2/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.3678 - val_loss: 2.3629\n","\n","Epoch 00002: val_loss improved from 2.39325 to 2.36287, saving model to best_model_2.h5\n","Epoch 3/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.3422 - val_loss: 2.3604\n","\n","Epoch 00003: val_loss improved from 2.36287 to 2.36040, saving model to best_model_2.h5\n","Epoch 4/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.3124 - val_loss: 2.3167\n","\n","Epoch 00004: val_loss improved from 2.36040 to 2.31671, saving model to best_model_2.h5\n","Epoch 5/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2778 - val_loss: 2.2807\n","\n","Epoch 00005: val_loss improved from 2.31671 to 2.28069, saving model to best_model_2.h5\n","Epoch 6/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2647 - val_loss: 2.2422\n","\n","Epoch 00006: val_loss improved from 2.28069 to 2.24219, saving model to best_model_2.h5\n","Epoch 7/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2191 - val_loss: 2.2083\n","\n","Epoch 00007: val_loss improved from 2.24219 to 2.20825, saving model to best_model_2.h5\n","Epoch 8/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.1848 - val_loss: 2.1758\n","\n","Epoch 00008: val_loss improved from 2.20825 to 2.17579, saving model to best_model_2.h5\n","Epoch 9/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1555 - val_loss: 2.1553\n","\n","Epoch 00009: val_loss improved from 2.17579 to 2.15526, saving model to best_model_2.h5\n","Epoch 10/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1283 - val_loss: 2.1135\n","\n","Epoch 00010: val_loss improved from 2.15526 to 2.11350, saving model to best_model_2.h5\n","Epoch 11/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.0996 - val_loss: 2.0817\n","\n","Epoch 00011: val_loss improved from 2.11350 to 2.08171, saving model to best_model_2.h5\n","Epoch 12/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.0617 - val_loss: 2.0475\n","\n","Epoch 00012: val_loss improved from 2.08171 to 2.04754, saving model to best_model_2.h5\n","Epoch 13/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.0313 - val_loss: 2.0145\n","\n","Epoch 00013: val_loss improved from 2.04754 to 2.01446, saving model to best_model_2.h5\n","Epoch 14/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.9848 - val_loss: 1.9850\n","\n","Epoch 00014: val_loss improved from 2.01446 to 1.98499, saving model to best_model_2.h5\n","Epoch 15/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.9639 - val_loss: 1.9464\n","\n","Epoch 00015: val_loss improved from 1.98499 to 1.94644, saving model to best_model_2.h5\n","Epoch 16/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.9175 - val_loss: 1.9949\n","\n","Epoch 00016: val_loss did not improve from 1.94644\n","Epoch 17/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.9019 - val_loss: 1.9243\n","\n","Epoch 00017: val_loss improved from 1.94644 to 1.92428, saving model to best_model_2.h5\n","Epoch 18/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.8645 - val_loss: 1.8487\n","\n","Epoch 00018: val_loss improved from 1.92428 to 1.84870, saving model to best_model_2.h5\n","Epoch 19/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.8206 - val_loss: 1.8128\n","\n","Epoch 00019: val_loss improved from 1.84870 to 1.81276, saving model to best_model_2.h5\n","Epoch 20/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7973 - val_loss: 1.8028\n","\n","Epoch 00020: val_loss improved from 1.81276 to 1.80280, saving model to best_model_2.h5\n","Epoch 21/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7628 - val_loss: 1.7549\n","\n","Epoch 00021: val_loss improved from 1.80280 to 1.75487, saving model to best_model_2.h5\n","Epoch 22/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7466 - val_loss: 1.7541\n","\n","Epoch 00022: val_loss improved from 1.75487 to 1.75411, saving model to best_model_2.h5\n","Epoch 23/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7001 - val_loss: 1.6940\n","\n","Epoch 00023: val_loss improved from 1.75411 to 1.69403, saving model to best_model_2.h5\n","Epoch 24/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.6677 - val_loss: 1.6653\n","\n","Epoch 00024: val_loss improved from 1.69403 to 1.66529, saving model to best_model_2.h5\n","Epoch 25/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.6407 - val_loss: 1.6354\n","\n","Epoch 00025: val_loss improved from 1.66529 to 1.63536, saving model to best_model_2.h5\n","Epoch 26/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.6201 - val_loss: 1.6100\n","\n","Epoch 00026: val_loss improved from 1.63536 to 1.60996, saving model to best_model_2.h5\n","Epoch 27/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.5881 - val_loss: 1.7028\n","\n","Epoch 00027: val_loss did not improve from 1.60996\n","Epoch 28/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.5710 - val_loss: 1.5638\n","\n","Epoch 00028: val_loss improved from 1.60996 to 1.56376, saving model to best_model_2.h5\n","Epoch 29/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.5311 - val_loss: 1.5325\n","\n","Epoch 00029: val_loss improved from 1.56376 to 1.53249, saving model to best_model_2.h5\n","Epoch 30/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.5116 - val_loss: 1.5272\n","\n","Epoch 00030: val_loss improved from 1.53249 to 1.52720, saving model to best_model_2.h5\n","==================== 0.3day1 ====================\n","Epoch 1/30\n","326/326 [==============================] - 6s 15ms/step - loss: 4.2740 - val_loss: 2.7245\n","\n","Epoch 00001: val_loss improved from inf to 2.72450, saving model to best_model_1.h5\n","Epoch 2/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.6816 - val_loss: 2.6909\n","\n","Epoch 00002: val_loss improved from 2.72450 to 2.69086, saving model to best_model_1.h5\n","Epoch 3/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6451 - val_loss: 2.6583\n","\n","Epoch 00003: val_loss improved from 2.69086 to 2.65826, saving model to best_model_1.h5\n","Epoch 4/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6347 - val_loss: 2.6514\n","\n","Epoch 00004: val_loss improved from 2.65826 to 2.65140, saving model to best_model_1.h5\n","Epoch 5/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6150 - val_loss: 2.6162\n","\n","Epoch 00005: val_loss improved from 2.65140 to 2.61621, saving model to best_model_1.h5\n","Epoch 6/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6023 - val_loss: 2.6124\n","\n","Epoch 00006: val_loss improved from 2.61621 to 2.61243, saving model to best_model_1.h5\n","Epoch 7/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5785 - val_loss: 2.5859\n","\n","Epoch 00007: val_loss improved from 2.61243 to 2.58586, saving model to best_model_1.h5\n","Epoch 8/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5729 - val_loss: 2.5686\n","\n","Epoch 00008: val_loss improved from 2.58586 to 2.56861, saving model to best_model_1.h5\n","Epoch 9/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5367 - val_loss: 2.5532\n","\n","Epoch 00009: val_loss improved from 2.56861 to 2.55319, saving model to best_model_1.h5\n","Epoch 10/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5277 - val_loss: 2.5289\n","\n","Epoch 00010: val_loss improved from 2.55319 to 2.52891, saving model to best_model_1.h5\n","Epoch 11/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5199 - val_loss: 2.5175\n","\n","Epoch 00011: val_loss improved from 2.52891 to 2.51746, saving model to best_model_1.h5\n","Epoch 12/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.4877 - val_loss: 2.4923\n","\n","Epoch 00012: val_loss improved from 2.51746 to 2.49234, saving model to best_model_1.h5\n","Epoch 13/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.4614 - val_loss: 2.4555\n","\n","Epoch 00013: val_loss improved from 2.49234 to 2.45549, saving model to best_model_1.h5\n","Epoch 14/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.4337 - val_loss: 2.4214\n","\n","Epoch 00014: val_loss improved from 2.45549 to 2.42142, saving model to best_model_1.h5\n","Epoch 15/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.3980 - val_loss: 2.4169\n","\n","Epoch 00015: val_loss improved from 2.42142 to 2.41689, saving model to best_model_1.h5\n","Epoch 16/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.3643 - val_loss: 2.3926\n","\n","Epoch 00016: val_loss improved from 2.41689 to 2.39260, saving model to best_model_1.h5\n","Epoch 17/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.3382 - val_loss: 2.3500\n","\n","Epoch 00017: val_loss improved from 2.39260 to 2.34996, saving model to best_model_1.h5\n","Epoch 18/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.3034 - val_loss: 2.2913\n","\n","Epoch 00018: val_loss improved from 2.34996 to 2.29132, saving model to best_model_1.h5\n","Epoch 19/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.2744 - val_loss: 2.2669\n","\n","Epoch 00019: val_loss improved from 2.29132 to 2.26694, saving model to best_model_1.h5\n","Epoch 20/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.2316 - val_loss: 2.2184\n","\n","Epoch 00020: val_loss improved from 2.26694 to 2.21836, saving model to best_model_1.h5\n","Epoch 21/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.2048 - val_loss: 2.2258\n","\n","Epoch 00021: val_loss did not improve from 2.21836\n","Epoch 22/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.1824 - val_loss: 2.1549\n","\n","Epoch 00022: val_loss improved from 2.21836 to 2.15492, saving model to best_model_1.h5\n","Epoch 23/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.1436 - val_loss: 2.1639\n","\n","Epoch 00023: val_loss did not improve from 2.15492\n","Epoch 24/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.1071 - val_loss: 2.1046\n","\n","Epoch 00024: val_loss improved from 2.15492 to 2.10456, saving model to best_model_1.h5\n","Epoch 25/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.0835 - val_loss: 2.0611\n","\n","Epoch 00025: val_loss improved from 2.10456 to 2.06114, saving model to best_model_1.h5\n","Epoch 26/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.0446 - val_loss: 2.0374\n","\n","Epoch 00026: val_loss improved from 2.06114 to 2.03743, saving model to best_model_1.h5\n","Epoch 27/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.0132 - val_loss: 2.0213\n","\n","Epoch 00027: val_loss improved from 2.03743 to 2.02127, saving model to best_model_1.h5\n","Epoch 28/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.9829 - val_loss: 1.9777\n","\n","Epoch 00028: val_loss improved from 2.02127 to 1.97773, saving model to best_model_1.h5\n","Epoch 29/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.9602 - val_loss: 1.9384\n","\n","Epoch 00029: val_loss improved from 1.97773 to 1.93837, saving model to best_model_1.h5\n","Epoch 30/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.9236 - val_loss: 1.9226\n","\n","Epoch 00030: val_loss improved from 1.93837 to 1.92256, saving model to best_model_1.h5\n","==================== 0.3day2 ====================\n","Epoch 1/30\n","326/326 [==============================] - 5s 14ms/step - loss: 4.3314 - val_loss: 2.8696\n","\n","Epoch 00001: val_loss improved from inf to 2.86959, saving model to best_model_2.h5\n","Epoch 2/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.8252 - val_loss: 2.8466\n","\n","Epoch 00002: val_loss improved from 2.86959 to 2.84663, saving model to best_model_2.h5\n","Epoch 3/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.8049 - val_loss: 2.8056\n","\n","Epoch 00003: val_loss improved from 2.84663 to 2.80563, saving model to best_model_2.h5\n","Epoch 4/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.7681 - val_loss: 2.7683\n","\n","Epoch 00004: val_loss improved from 2.80563 to 2.76831, saving model to best_model_2.h5\n","Epoch 5/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.7407 - val_loss: 2.7338\n","\n","Epoch 00005: val_loss improved from 2.76831 to 2.73380, saving model to best_model_2.h5\n","Epoch 6/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6932 - val_loss: 2.7086\n","\n","Epoch 00006: val_loss improved from 2.73380 to 2.70864, saving model to best_model_2.h5\n","Epoch 7/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6820 - val_loss: 2.6778\n","\n","Epoch 00007: val_loss improved from 2.70864 to 2.67783, saving model to best_model_2.h5\n","Epoch 8/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6557 - val_loss: 2.6530\n","\n","Epoch 00008: val_loss improved from 2.67783 to 2.65300, saving model to best_model_2.h5\n","Epoch 9/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6380 - val_loss: 2.6790\n","\n","Epoch 00009: val_loss did not improve from 2.65300\n","Epoch 10/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6047 - val_loss: 2.6074\n","\n","Epoch 00010: val_loss improved from 2.65300 to 2.60737, saving model to best_model_2.h5\n","Epoch 11/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5848 - val_loss: 2.5632\n","\n","Epoch 00011: val_loss improved from 2.60737 to 2.56324, saving model to best_model_2.h5\n","Epoch 12/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5312 - val_loss: 2.5318\n","\n","Epoch 00012: val_loss improved from 2.56324 to 2.53177, saving model to best_model_2.h5\n","Epoch 13/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5203 - val_loss: 2.5032\n","\n","Epoch 00013: val_loss improved from 2.53177 to 2.50318, saving model to best_model_2.h5\n","Epoch 14/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.4767 - val_loss: 2.4606\n","\n","Epoch 00014: val_loss improved from 2.50318 to 2.46063, saving model to best_model_2.h5\n","Epoch 15/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.4420 - val_loss: 2.4434\n","\n","Epoch 00015: val_loss improved from 2.46063 to 2.44338, saving model to best_model_2.h5\n","Epoch 16/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.4026 - val_loss: 2.3938\n","\n","Epoch 00016: val_loss improved from 2.44338 to 2.39377, saving model to best_model_2.h5\n","Epoch 17/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.3711 - val_loss: 2.3555\n","\n","Epoch 00017: val_loss improved from 2.39377 to 2.35546, saving model to best_model_2.h5\n","Epoch 18/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.3597 - val_loss: 2.3247\n","\n","Epoch 00018: val_loss improved from 2.35546 to 2.32473, saving model to best_model_2.h5\n","Epoch 19/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.3053 - val_loss: 2.2918\n","\n","Epoch 00019: val_loss improved from 2.32473 to 2.29181, saving model to best_model_2.h5\n","Epoch 20/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.2611 - val_loss: 2.3034\n","\n","Epoch 00020: val_loss did not improve from 2.29181\n","Epoch 21/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.2401 - val_loss: 2.2344\n","\n","Epoch 00021: val_loss improved from 2.29181 to 2.23440, saving model to best_model_2.h5\n","Epoch 22/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.2149 - val_loss: 2.2001\n","\n","Epoch 00022: val_loss improved from 2.23440 to 2.20006, saving model to best_model_2.h5\n","Epoch 23/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.1546 - val_loss: 2.1613\n","\n","Epoch 00023: val_loss improved from 2.20006 to 2.16129, saving model to best_model_2.h5\n","Epoch 24/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.1440 - val_loss: 2.1414\n","\n","Epoch 00024: val_loss improved from 2.16129 to 2.14144, saving model to best_model_2.h5\n","Epoch 25/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.1067 - val_loss: 2.1105\n","\n","Epoch 00025: val_loss improved from 2.14144 to 2.11047, saving model to best_model_2.h5\n","Epoch 26/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.0631 - val_loss: 2.0650\n","\n","Epoch 00026: val_loss improved from 2.11047 to 2.06504, saving model to best_model_2.h5\n","Epoch 27/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.0390 - val_loss: 2.0382\n","\n","Epoch 00027: val_loss improved from 2.06504 to 2.03820, saving model to best_model_2.h5\n","Epoch 28/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.0026 - val_loss: 2.0166\n","\n","Epoch 00028: val_loss improved from 2.03820 to 2.01662, saving model to best_model_2.h5\n","Epoch 29/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.9629 - val_loss: 1.9665\n","\n","Epoch 00029: val_loss improved from 2.01662 to 1.96647, saving model to best_model_2.h5\n","Epoch 30/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.9433 - val_loss: 1.9335\n","\n","Epoch 00030: val_loss improved from 1.96647 to 1.93346, saving model to best_model_2.h5\n","==================== 0.4day1 ====================\n","Epoch 1/30\n","326/326 [==============================] - 6s 15ms/step - loss: 5.1454 - val_loss: 2.8687\n","\n","Epoch 00001: val_loss improved from inf to 2.86866, saving model to best_model_1.h5\n","Epoch 2/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.8403 - val_loss: 2.8043\n","\n","Epoch 00002: val_loss improved from 2.86866 to 2.80426, saving model to best_model_1.h5\n","Epoch 3/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.7986 - val_loss: 2.7851\n","\n","Epoch 00003: val_loss improved from 2.80426 to 2.78508, saving model to best_model_1.h5\n","Epoch 4/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.7472 - val_loss: 2.7535\n","\n","Epoch 00004: val_loss improved from 2.78508 to 2.75352, saving model to best_model_1.h5\n","Epoch 5/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.7323 - val_loss: 2.7443\n","\n","Epoch 00005: val_loss improved from 2.75352 to 2.74432, saving model to best_model_1.h5\n","Epoch 6/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.7008 - val_loss: 2.7110\n","\n","Epoch 00006: val_loss improved from 2.74432 to 2.71104, saving model to best_model_1.h5\n","Epoch 7/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.6888 - val_loss: 2.6929\n","\n","Epoch 00007: val_loss improved from 2.71104 to 2.69286, saving model to best_model_1.h5\n","Epoch 8/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.6780 - val_loss: 2.6772\n","\n","Epoch 00008: val_loss improved from 2.69286 to 2.67724, saving model to best_model_1.h5\n","Epoch 9/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.6434 - val_loss: 2.6572\n","\n","Epoch 00009: val_loss improved from 2.67724 to 2.65722, saving model to best_model_1.h5\n","Epoch 10/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.6376 - val_loss: 2.6413\n","\n","Epoch 00010: val_loss improved from 2.65722 to 2.64134, saving model to best_model_1.h5\n","Epoch 11/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.6078 - val_loss: 2.6865\n","\n","Epoch 00011: val_loss did not improve from 2.64134\n","Epoch 12/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.5903 - val_loss: 2.6237\n","\n","Epoch 00012: val_loss improved from 2.64134 to 2.62369, saving model to best_model_1.h5\n","Epoch 13/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.5792 - val_loss: 2.5672\n","\n","Epoch 00013: val_loss improved from 2.62369 to 2.56716, saving model to best_model_1.h5\n","Epoch 14/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.5428 - val_loss: 2.5346\n","\n","Epoch 00014: val_loss improved from 2.56716 to 2.53456, saving model to best_model_1.h5\n","Epoch 15/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.5110 - val_loss: 2.5041\n","\n","Epoch 00015: val_loss improved from 2.53456 to 2.50408, saving model to best_model_1.h5\n","Epoch 16/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.4661 - val_loss: 2.4747\n","\n","Epoch 00016: val_loss improved from 2.50408 to 2.47473, saving model to best_model_1.h5\n","Epoch 17/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.4331 - val_loss: 2.5532\n","\n","Epoch 00017: val_loss did not improve from 2.47473\n","Epoch 18/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3964 - val_loss: 2.4013\n","\n","Epoch 00018: val_loss improved from 2.47473 to 2.40132, saving model to best_model_1.h5\n","Epoch 19/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3674 - val_loss: 2.3781\n","\n","Epoch 00019: val_loss improved from 2.40132 to 2.37814, saving model to best_model_1.h5\n","Epoch 20/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3322 - val_loss: 2.3390\n","\n","Epoch 00020: val_loss improved from 2.37814 to 2.33903, saving model to best_model_1.h5\n","Epoch 21/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2715 - val_loss: 2.2784\n","\n","Epoch 00021: val_loss improved from 2.33903 to 2.27840, saving model to best_model_1.h5\n","Epoch 22/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2686 - val_loss: 2.2376\n","\n","Epoch 00022: val_loss improved from 2.27840 to 2.23755, saving model to best_model_1.h5\n","Epoch 23/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2061 - val_loss: 2.2115\n","\n","Epoch 00023: val_loss improved from 2.23755 to 2.21150, saving model to best_model_1.h5\n","Epoch 24/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.1771 - val_loss: 2.1809\n","\n","Epoch 00024: val_loss improved from 2.21150 to 2.18095, saving model to best_model_1.h5\n","Epoch 25/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.1240 - val_loss: 2.1486\n","\n","Epoch 00025: val_loss improved from 2.18095 to 2.14861, saving model to best_model_1.h5\n","Epoch 26/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.1127 - val_loss: 2.1373\n","\n","Epoch 00026: val_loss improved from 2.14861 to 2.13727, saving model to best_model_1.h5\n","Epoch 27/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.0687 - val_loss: 2.0744\n","\n","Epoch 00027: val_loss improved from 2.13727 to 2.07438, saving model to best_model_1.h5\n","Epoch 28/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.0265 - val_loss: 2.0516\n","\n","Epoch 00028: val_loss improved from 2.07438 to 2.05161, saving model to best_model_1.h5\n","Epoch 29/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.0116 - val_loss: 2.0352\n","\n","Epoch 00029: val_loss improved from 2.05161 to 2.03519, saving model to best_model_1.h5\n","Epoch 30/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.9832 - val_loss: 1.9990\n","\n","Epoch 00030: val_loss improved from 2.03519 to 1.99901, saving model to best_model_1.h5\n","==================== 0.4day2 ====================\n","Epoch 1/30\n","326/326 [==============================] - 5s 14ms/step - loss: 5.4456 - val_loss: 2.9964\n","\n","Epoch 00001: val_loss improved from inf to 2.99645, saving model to best_model_2.h5\n","Epoch 2/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.9483 - val_loss: 2.9493\n","\n","Epoch 00002: val_loss improved from 2.99645 to 2.94935, saving model to best_model_2.h5\n","Epoch 3/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.9081 - val_loss: 2.9248\n","\n","Epoch 00003: val_loss improved from 2.94935 to 2.92482, saving model to best_model_2.h5\n","Epoch 4/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.8931 - val_loss: 2.9050\n","\n","Epoch 00004: val_loss improved from 2.92482 to 2.90504, saving model to best_model_2.h5\n","Epoch 5/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.8707 - val_loss: 2.8940\n","\n","Epoch 00005: val_loss improved from 2.90504 to 2.89402, saving model to best_model_2.h5\n","Epoch 6/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.8811 - val_loss: 2.8701\n","\n","Epoch 00006: val_loss improved from 2.89402 to 2.87008, saving model to best_model_2.h5\n","Epoch 7/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.8476 - val_loss: 2.8579\n","\n","Epoch 00007: val_loss improved from 2.87008 to 2.85787, saving model to best_model_2.h5\n","Epoch 8/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.8279 - val_loss: 2.8514\n","\n","Epoch 00008: val_loss improved from 2.85787 to 2.85138, saving model to best_model_2.h5\n","Epoch 9/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.8068 - val_loss: 2.8187\n","\n","Epoch 00009: val_loss improved from 2.85138 to 2.81872, saving model to best_model_2.h5\n","Epoch 10/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.8007 - val_loss: 2.8041\n","\n","Epoch 00010: val_loss improved from 2.81872 to 2.80411, saving model to best_model_2.h5\n","Epoch 11/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.7788 - val_loss: 2.8283\n","\n","Epoch 00011: val_loss did not improve from 2.80411\n","Epoch 12/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.7738 - val_loss: 2.7832\n","\n","Epoch 00012: val_loss improved from 2.80411 to 2.78320, saving model to best_model_2.h5\n","Epoch 13/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.7295 - val_loss: 2.7626\n","\n","Epoch 00013: val_loss improved from 2.78320 to 2.76259, saving model to best_model_2.h5\n","Epoch 14/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.7238 - val_loss: 2.7421\n","\n","Epoch 00014: val_loss improved from 2.76259 to 2.74210, saving model to best_model_2.h5\n","Epoch 15/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6928 - val_loss: 2.6958\n","\n","Epoch 00015: val_loss improved from 2.74210 to 2.69580, saving model to best_model_2.h5\n","Epoch 16/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6716 - val_loss: 2.6789\n","\n","Epoch 00016: val_loss improved from 2.69580 to 2.67888, saving model to best_model_2.h5\n","Epoch 17/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6327 - val_loss: 2.6438\n","\n","Epoch 00017: val_loss improved from 2.67888 to 2.64377, saving model to best_model_2.h5\n","Epoch 18/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.6147 - val_loss: 2.6229\n","\n","Epoch 00018: val_loss improved from 2.64377 to 2.62291, saving model to best_model_2.h5\n","Epoch 19/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5805 - val_loss: 2.6063\n","\n","Epoch 00019: val_loss improved from 2.62291 to 2.60627, saving model to best_model_2.h5\n","Epoch 20/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5639 - val_loss: 2.5698\n","\n","Epoch 00020: val_loss improved from 2.60627 to 2.56979, saving model to best_model_2.h5\n","Epoch 21/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5416 - val_loss: 2.5455\n","\n","Epoch 00021: val_loss improved from 2.56979 to 2.54550, saving model to best_model_2.h5\n","Epoch 22/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5225 - val_loss: 2.5211\n","\n","Epoch 00022: val_loss improved from 2.54550 to 2.52113, saving model to best_model_2.h5\n","Epoch 23/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.4898 - val_loss: 2.4967\n","\n","Epoch 00023: val_loss improved from 2.52113 to 2.49674, saving model to best_model_2.h5\n","Epoch 24/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.4713 - val_loss: 2.4774\n","\n","Epoch 00024: val_loss improved from 2.49674 to 2.47741, saving model to best_model_2.h5\n","Epoch 25/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.4609 - val_loss: 2.4664\n","\n","Epoch 00025: val_loss improved from 2.47741 to 2.46639, saving model to best_model_2.h5\n","Epoch 26/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.4189 - val_loss: 2.4942\n","\n","Epoch 00026: val_loss did not improve from 2.46639\n","Epoch 27/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.4011 - val_loss: 2.4013\n","\n","Epoch 00027: val_loss improved from 2.46639 to 2.40130, saving model to best_model_2.h5\n","Epoch 28/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.3825 - val_loss: 2.3647\n","\n","Epoch 00028: val_loss improved from 2.40130 to 2.36474, saving model to best_model_2.h5\n","Epoch 29/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.3357 - val_loss: 2.3530\n","\n","Epoch 00029: val_loss improved from 2.36474 to 2.35297, saving model to best_model_2.h5\n","Epoch 30/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.3172 - val_loss: 2.3185\n","\n","Epoch 00030: val_loss improved from 2.35297 to 2.31847, saving model to best_model_2.h5\n","==================== 0.5day1 ====================\n","Epoch 1/30\n","326/326 [==============================] - 6s 15ms/step - loss: 6.0421 - val_loss: 2.7423\n","\n","Epoch 00001: val_loss improved from inf to 2.74229, saving model to best_model_1.h5\n","Epoch 2/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.7068 - val_loss: 2.6933\n","\n","Epoch 00002: val_loss improved from 2.74229 to 2.69334, saving model to best_model_1.h5\n","Epoch 3/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.6614 - val_loss: 2.6681\n","\n","Epoch 00003: val_loss improved from 2.69334 to 2.66808, saving model to best_model_1.h5\n","Epoch 4/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.6449 - val_loss: 2.6584\n","\n","Epoch 00004: val_loss improved from 2.66808 to 2.65842, saving model to best_model_1.h5\n","Epoch 5/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.6228 - val_loss: 2.6344\n","\n","Epoch 00005: val_loss improved from 2.65842 to 2.63435, saving model to best_model_1.h5\n","Epoch 6/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.6138 - val_loss: 2.6237\n","\n","Epoch 00006: val_loss improved from 2.63435 to 2.62372, saving model to best_model_1.h5\n","Epoch 7/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.6067 - val_loss: 2.6177\n","\n","Epoch 00007: val_loss improved from 2.62372 to 2.61770, saving model to best_model_1.h5\n","Epoch 8/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.5828 - val_loss: 2.6378\n","\n","Epoch 00008: val_loss did not improve from 2.61770\n","Epoch 9/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.5664 - val_loss: 2.5996\n","\n","Epoch 00009: val_loss improved from 2.61770 to 2.59959, saving model to best_model_1.h5\n","Epoch 10/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.5757 - val_loss: 2.6106\n","\n","Epoch 00010: val_loss did not improve from 2.59959\n","Epoch 11/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.5679 - val_loss: 2.5819\n","\n","Epoch 00011: val_loss improved from 2.59959 to 2.58188, saving model to best_model_1.h5\n","Epoch 12/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.5566 - val_loss: 2.6017\n","\n","Epoch 00012: val_loss did not improve from 2.58188\n","Epoch 13/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.5217 - val_loss: 2.5661\n","\n","Epoch 00013: val_loss improved from 2.58188 to 2.56609, saving model to best_model_1.h5\n","Epoch 14/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.5225 - val_loss: 2.5764\n","\n","Epoch 00014: val_loss did not improve from 2.56609\n","Epoch 15/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.5209 - val_loss: 2.5452\n","\n","Epoch 00015: val_loss improved from 2.56609 to 2.54517, saving model to best_model_1.h5\n","Epoch 16/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.4898 - val_loss: 2.5295\n","\n","Epoch 00016: val_loss improved from 2.54517 to 2.52950, saving model to best_model_1.h5\n","Epoch 17/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.4862 - val_loss: 2.5033\n","\n","Epoch 00017: val_loss improved from 2.52950 to 2.50327, saving model to best_model_1.h5\n","Epoch 18/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.4924 - val_loss: 2.4878\n","\n","Epoch 00018: val_loss improved from 2.50327 to 2.48785, saving model to best_model_1.h5\n","Epoch 19/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.4542 - val_loss: 2.4767\n","\n","Epoch 00019: val_loss improved from 2.48785 to 2.47671, saving model to best_model_1.h5\n","Epoch 20/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.4321 - val_loss: 2.4801\n","\n","Epoch 00020: val_loss did not improve from 2.47671\n","Epoch 21/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.4177 - val_loss: 2.4323\n","\n","Epoch 00021: val_loss improved from 2.47671 to 2.43229, saving model to best_model_1.h5\n","Epoch 22/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3843 - val_loss: 2.4139\n","\n","Epoch 00022: val_loss improved from 2.43229 to 2.41387, saving model to best_model_1.h5\n","Epoch 23/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3503 - val_loss: 2.4107\n","\n","Epoch 00023: val_loss improved from 2.41387 to 2.41066, saving model to best_model_1.h5\n","Epoch 24/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3553 - val_loss: 2.3538\n","\n","Epoch 00024: val_loss improved from 2.41066 to 2.35375, saving model to best_model_1.h5\n","Epoch 25/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3128 - val_loss: 2.3261\n","\n","Epoch 00025: val_loss improved from 2.35375 to 2.32613, saving model to best_model_1.h5\n","Epoch 26/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2932 - val_loss: 2.2970\n","\n","Epoch 00026: val_loss improved from 2.32613 to 2.29698, saving model to best_model_1.h5\n","Epoch 27/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2608 - val_loss: 2.2745\n","\n","Epoch 00027: val_loss improved from 2.29698 to 2.27449, saving model to best_model_1.h5\n","Epoch 28/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2407 - val_loss: 2.3033\n","\n","Epoch 00028: val_loss did not improve from 2.27449\n","Epoch 29/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2216 - val_loss: 2.2224\n","\n","Epoch 00029: val_loss improved from 2.27449 to 2.22242, saving model to best_model_1.h5\n","Epoch 30/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.1865 - val_loss: 2.1905\n","\n","Epoch 00030: val_loss improved from 2.22242 to 2.19047, saving model to best_model_1.h5\n","==================== 0.5day2 ====================\n","Epoch 1/30\n","326/326 [==============================] - 5s 14ms/step - loss: 6.1396 - val_loss: 2.8536\n","\n","Epoch 00001: val_loss improved from inf to 2.85361, saving model to best_model_2.h5\n","Epoch 2/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.7973 - val_loss: 2.8170\n","\n","Epoch 00002: val_loss improved from 2.85361 to 2.81700, saving model to best_model_2.h5\n","Epoch 3/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.7816 - val_loss: 2.7956\n","\n","Epoch 00003: val_loss improved from 2.81700 to 2.79559, saving model to best_model_2.h5\n","Epoch 4/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.7664 - val_loss: 2.7804\n","\n","Epoch 00004: val_loss improved from 2.79559 to 2.78039, saving model to best_model_2.h5\n","Epoch 5/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.7487 - val_loss: 2.7544\n","\n","Epoch 00005: val_loss improved from 2.78039 to 2.75436, saving model to best_model_2.h5\n","Epoch 6/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.7248 - val_loss: 2.7354\n","\n","Epoch 00006: val_loss improved from 2.75436 to 2.73541, saving model to best_model_2.h5\n","Epoch 7/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.7061 - val_loss: 2.7208\n","\n","Epoch 00007: val_loss improved from 2.73541 to 2.72079, saving model to best_model_2.h5\n","Epoch 8/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.6904 - val_loss: 2.7045\n","\n","Epoch 00008: val_loss improved from 2.72079 to 2.70454, saving model to best_model_2.h5\n","Epoch 9/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.6653 - val_loss: 2.6841\n","\n","Epoch 00009: val_loss improved from 2.70454 to 2.68412, saving model to best_model_2.h5\n","Epoch 10/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.6503 - val_loss: 2.6913\n","\n","Epoch 00010: val_loss did not improve from 2.68412\n","Epoch 11/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.6461 - val_loss: 2.6582\n","\n","Epoch 00011: val_loss improved from 2.68412 to 2.65815, saving model to best_model_2.h5\n","Epoch 12/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.6411 - val_loss: 2.6485\n","\n","Epoch 00012: val_loss improved from 2.65815 to 2.64849, saving model to best_model_2.h5\n","Epoch 13/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.6221 - val_loss: 2.6292\n","\n","Epoch 00013: val_loss improved from 2.64849 to 2.62917, saving model to best_model_2.h5\n","Epoch 14/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.6037 - val_loss: 2.6347\n","\n","Epoch 00014: val_loss did not improve from 2.62917\n","Epoch 15/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.5784 - val_loss: 2.6010\n","\n","Epoch 00015: val_loss improved from 2.62917 to 2.60099, saving model to best_model_2.h5\n","Epoch 16/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.5550 - val_loss: 2.5817\n","\n","Epoch 00016: val_loss improved from 2.60099 to 2.58166, saving model to best_model_2.h5\n","Epoch 17/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.5401 - val_loss: 2.5602\n","\n","Epoch 00017: val_loss improved from 2.58166 to 2.56016, saving model to best_model_2.h5\n","Epoch 18/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.5294 - val_loss: 2.5390\n","\n","Epoch 00018: val_loss improved from 2.56016 to 2.53895, saving model to best_model_2.h5\n","Epoch 19/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.5028 - val_loss: 2.5185\n","\n","Epoch 00019: val_loss improved from 2.53895 to 2.51847, saving model to best_model_2.h5\n","Epoch 20/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.4773 - val_loss: 2.4986\n","\n","Epoch 00020: val_loss improved from 2.51847 to 2.49862, saving model to best_model_2.h5\n","Epoch 21/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.4596 - val_loss: 2.4879\n","\n","Epoch 00021: val_loss improved from 2.49862 to 2.48787, saving model to best_model_2.h5\n","Epoch 22/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.4364 - val_loss: 2.4474\n","\n","Epoch 00022: val_loss improved from 2.48787 to 2.44738, saving model to best_model_2.h5\n","Epoch 23/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.4285 - val_loss: 2.4282\n","\n","Epoch 00023: val_loss improved from 2.44738 to 2.42818, saving model to best_model_2.h5\n","Epoch 24/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.4004 - val_loss: 2.4410\n","\n","Epoch 00024: val_loss did not improve from 2.42818\n","Epoch 25/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.3943 - val_loss: 2.4016\n","\n","Epoch 00025: val_loss improved from 2.42818 to 2.40162, saving model to best_model_2.h5\n","Epoch 26/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.3546 - val_loss: 2.3604\n","\n","Epoch 00026: val_loss improved from 2.40162 to 2.36037, saving model to best_model_2.h5\n","Epoch 27/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.3287 - val_loss: 2.3841\n","\n","Epoch 00027: val_loss did not improve from 2.36037\n","Epoch 28/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2969 - val_loss: 2.3196\n","\n","Epoch 00028: val_loss improved from 2.36037 to 2.31957, saving model to best_model_2.h5\n","Epoch 29/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2978 - val_loss: 2.3245\n","\n","Epoch 00029: val_loss did not improve from 2.31957\n","Epoch 30/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2713 - val_loss: 2.2695\n","\n","Epoch 00030: val_loss improved from 2.31957 to 2.26954, saving model to best_model_2.h5\n","==================== 0.6day1 ====================\n","Epoch 1/30\n","326/326 [==============================] - 6s 14ms/step - loss: 6.9878 - val_loss: 2.4959\n","\n","Epoch 00001: val_loss improved from inf to 2.49587, saving model to best_model_1.h5\n","Epoch 2/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.4427 - val_loss: 2.4303\n","\n","Epoch 00002: val_loss improved from 2.49587 to 2.43030, saving model to best_model_1.h5\n","Epoch 3/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.3937 - val_loss: 2.3823\n","\n","Epoch 00003: val_loss improved from 2.43030 to 2.38233, saving model to best_model_1.h5\n","Epoch 4/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.3610 - val_loss: 2.3674\n","\n","Epoch 00004: val_loss improved from 2.38233 to 2.36737, saving model to best_model_1.h5\n","Epoch 5/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.3214 - val_loss: 2.3437\n","\n","Epoch 00005: val_loss improved from 2.36737 to 2.34374, saving model to best_model_1.h5\n","Epoch 6/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.3182 - val_loss: 2.3261\n","\n","Epoch 00006: val_loss improved from 2.34374 to 2.32607, saving model to best_model_1.h5\n","Epoch 7/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2847 - val_loss: 2.3343\n","\n","Epoch 00007: val_loss did not improve from 2.32607\n","Epoch 8/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2893 - val_loss: 2.3335\n","\n","Epoch 00008: val_loss did not improve from 2.32607\n","Epoch 9/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2847 - val_loss: 2.2998\n","\n","Epoch 00009: val_loss improved from 2.32607 to 2.29978, saving model to best_model_1.h5\n","Epoch 10/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2808 - val_loss: 2.2940\n","\n","Epoch 00010: val_loss improved from 2.29978 to 2.29397, saving model to best_model_1.h5\n","Epoch 11/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2758 - val_loss: 2.2963\n","\n","Epoch 00011: val_loss did not improve from 2.29397\n","Epoch 12/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2671 - val_loss: 2.2903\n","\n","Epoch 00012: val_loss improved from 2.29397 to 2.29027, saving model to best_model_1.h5\n","Epoch 13/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2448 - val_loss: 2.2859\n","\n","Epoch 00013: val_loss improved from 2.29027 to 2.28592, saving model to best_model_1.h5\n","Epoch 14/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2443 - val_loss: 2.2743\n","\n","Epoch 00014: val_loss improved from 2.28592 to 2.27427, saving model to best_model_1.h5\n","Epoch 15/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2290 - val_loss: 2.2665\n","\n","Epoch 00015: val_loss improved from 2.27427 to 2.26648, saving model to best_model_1.h5\n","Epoch 16/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2436 - val_loss: 2.2731\n","\n","Epoch 00016: val_loss did not improve from 2.26648\n","Epoch 17/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2273 - val_loss: 2.2624\n","\n","Epoch 00017: val_loss improved from 2.26648 to 2.26242, saving model to best_model_1.h5\n","Epoch 18/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2194 - val_loss: 2.2505\n","\n","Epoch 00018: val_loss improved from 2.26242 to 2.25053, saving model to best_model_1.h5\n","Epoch 19/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2171 - val_loss: 2.2438\n","\n","Epoch 00019: val_loss improved from 2.25053 to 2.24380, saving model to best_model_1.h5\n","Epoch 20/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2072 - val_loss: 2.2525\n","\n","Epoch 00020: val_loss did not improve from 2.24380\n","Epoch 21/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.2169 - val_loss: 2.2327\n","\n","Epoch 00021: val_loss improved from 2.24380 to 2.23273, saving model to best_model_1.h5\n","Epoch 22/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1823 - val_loss: 2.2482\n","\n","Epoch 00022: val_loss did not improve from 2.23273\n","Epoch 23/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1817 - val_loss: 2.2281\n","\n","Epoch 00023: val_loss improved from 2.23273 to 2.22809, saving model to best_model_1.h5\n","Epoch 24/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1745 - val_loss: 2.2087\n","\n","Epoch 00024: val_loss improved from 2.22809 to 2.20865, saving model to best_model_1.h5\n","Epoch 25/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1723 - val_loss: 2.2066\n","\n","Epoch 00025: val_loss improved from 2.20865 to 2.20659, saving model to best_model_1.h5\n","Epoch 26/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1867 - val_loss: 2.1997\n","\n","Epoch 00026: val_loss improved from 2.20659 to 2.19974, saving model to best_model_1.h5\n","Epoch 27/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1556 - val_loss: 2.2276\n","\n","Epoch 00027: val_loss did not improve from 2.19974\n","Epoch 28/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1659 - val_loss: 2.1793\n","\n","Epoch 00028: val_loss improved from 2.19974 to 2.17932, saving model to best_model_1.h5\n","Epoch 29/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1497 - val_loss: 2.1710\n","\n","Epoch 00029: val_loss improved from 2.17932 to 2.17098, saving model to best_model_1.h5\n","Epoch 30/30\n","326/326 [==============================] - 4s 14ms/step - loss: 2.1464 - val_loss: 2.1720\n","\n","Epoch 00030: val_loss did not improve from 2.17098\n","==================== 0.6day2 ====================\n","Epoch 1/30\n","326/326 [==============================] - 6s 15ms/step - loss: 7.4123 - val_loss: 2.5471\n","\n","Epoch 00001: val_loss improved from inf to 2.54710, saving model to best_model_2.h5\n","Epoch 2/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.4846 - val_loss: 2.4761\n","\n","Epoch 00002: val_loss improved from 2.54710 to 2.47608, saving model to best_model_2.h5\n","Epoch 3/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.4425 - val_loss: 2.4418\n","\n","Epoch 00003: val_loss improved from 2.47608 to 2.44175, saving model to best_model_2.h5\n","Epoch 4/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.4225 - val_loss: 2.4636\n","\n","Epoch 00004: val_loss did not improve from 2.44175\n","Epoch 5/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3962 - val_loss: 2.4051\n","\n","Epoch 00005: val_loss improved from 2.44175 to 2.40509, saving model to best_model_2.h5\n","Epoch 6/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3900 - val_loss: 2.3950\n","\n","Epoch 00006: val_loss improved from 2.40509 to 2.39502, saving model to best_model_2.h5\n","Epoch 7/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3714 - val_loss: 2.3946\n","\n","Epoch 00007: val_loss improved from 2.39502 to 2.39462, saving model to best_model_2.h5\n","Epoch 8/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3704 - val_loss: 2.3669\n","\n","Epoch 00008: val_loss improved from 2.39462 to 2.36691, saving model to best_model_2.h5\n","Epoch 9/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3424 - val_loss: 2.3803\n","\n","Epoch 00009: val_loss did not improve from 2.36691\n","Epoch 10/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3286 - val_loss: 2.3565\n","\n","Epoch 00010: val_loss improved from 2.36691 to 2.35646, saving model to best_model_2.h5\n","Epoch 11/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3240 - val_loss: 2.3453\n","\n","Epoch 00011: val_loss improved from 2.35646 to 2.34530, saving model to best_model_2.h5\n","Epoch 12/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3313 - val_loss: 2.3441\n","\n","Epoch 00012: val_loss improved from 2.34530 to 2.34406, saving model to best_model_2.h5\n","Epoch 13/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2949 - val_loss: 2.3505\n","\n","Epoch 00013: val_loss did not improve from 2.34406\n","Epoch 14/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.3011 - val_loss: 2.3222\n","\n","Epoch 00014: val_loss improved from 2.34406 to 2.32217, saving model to best_model_2.h5\n","Epoch 15/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2975 - val_loss: 2.3105\n","\n","Epoch 00015: val_loss improved from 2.32217 to 2.31046, saving model to best_model_2.h5\n","Epoch 16/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2782 - val_loss: 2.3044\n","\n","Epoch 00016: val_loss improved from 2.31046 to 2.30440, saving model to best_model_2.h5\n","Epoch 17/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2706 - val_loss: 2.3088\n","\n","Epoch 00017: val_loss did not improve from 2.30440\n","Epoch 18/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2604 - val_loss: 2.2901\n","\n","Epoch 00018: val_loss improved from 2.30440 to 2.29009, saving model to best_model_2.h5\n","Epoch 19/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2545 - val_loss: 2.2890\n","\n","Epoch 00019: val_loss improved from 2.29009 to 2.28899, saving model to best_model_2.h5\n","Epoch 20/30\n","326/326 [==============================] - 5s 15ms/step - loss: 2.2752 - val_loss: 2.2717\n","\n","Epoch 00020: val_loss improved from 2.28899 to 2.27169, saving model to best_model_2.h5\n","Epoch 21/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2316 - val_loss: 2.2955\n","\n","Epoch 00021: val_loss did not improve from 2.27169\n","Epoch 22/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2245 - val_loss: 2.2648\n","\n","Epoch 00022: val_loss improved from 2.27169 to 2.26484, saving model to best_model_2.h5\n","Epoch 23/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2252 - val_loss: 2.2373\n","\n","Epoch 00023: val_loss improved from 2.26484 to 2.23730, saving model to best_model_2.h5\n","Epoch 24/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.1982 - val_loss: 2.2286\n","\n","Epoch 00024: val_loss improved from 2.23730 to 2.22865, saving model to best_model_2.h5\n","Epoch 25/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.2114 - val_loss: 2.2189\n","\n","Epoch 00025: val_loss improved from 2.22865 to 2.21895, saving model to best_model_2.h5\n","Epoch 26/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.1904 - val_loss: 2.2100\n","\n","Epoch 00026: val_loss improved from 2.21895 to 2.20996, saving model to best_model_2.h5\n","Epoch 27/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.1794 - val_loss: 2.2019\n","\n","Epoch 00027: val_loss improved from 2.20996 to 2.20194, saving model to best_model_2.h5\n","Epoch 28/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.1804 - val_loss: 2.1966\n","\n","Epoch 00028: val_loss improved from 2.20194 to 2.19659, saving model to best_model_2.h5\n","Epoch 29/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.1528 - val_loss: 2.1819\n","\n","Epoch 00029: val_loss improved from 2.19659 to 2.18187, saving model to best_model_2.h5\n","Epoch 30/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.1443 - val_loss: 2.1928\n","\n","Epoch 00030: val_loss did not improve from 2.18187\n","==================== 0.7day1 ====================\n","Epoch 1/30\n","326/326 [==============================] - 5s 14ms/step - loss: 7.5768 - val_loss: 2.0854\n","\n","Epoch 00001: val_loss improved from inf to 2.08542, saving model to best_model_1.h5\n","Epoch 2/30\n","326/326 [==============================] - 4s 13ms/step - loss: 2.0208 - val_loss: 2.0133\n","\n","Epoch 00002: val_loss improved from 2.08542 to 2.01332, saving model to best_model_1.h5\n","Epoch 3/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.9778 - val_loss: 1.9685\n","\n","Epoch 00003: val_loss improved from 2.01332 to 1.96854, saving model to best_model_1.h5\n","Epoch 4/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.9365 - val_loss: 1.9410\n","\n","Epoch 00004: val_loss improved from 1.96854 to 1.94099, saving model to best_model_1.h5\n","Epoch 5/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.9068 - val_loss: 1.9135\n","\n","Epoch 00005: val_loss improved from 1.94099 to 1.91350, saving model to best_model_1.h5\n","Epoch 6/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.8840 - val_loss: 1.8929\n","\n","Epoch 00006: val_loss improved from 1.91350 to 1.89290, saving model to best_model_1.h5\n","Epoch 7/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.8628 - val_loss: 1.8901\n","\n","Epoch 00007: val_loss improved from 1.89290 to 1.89015, saving model to best_model_1.h5\n","Epoch 8/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.8490 - val_loss: 1.8740\n","\n","Epoch 00008: val_loss improved from 1.89015 to 1.87401, saving model to best_model_1.h5\n","Epoch 9/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.8311 - val_loss: 1.8619\n","\n","Epoch 00009: val_loss improved from 1.87401 to 1.86190, saving model to best_model_1.h5\n","Epoch 10/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.8295 - val_loss: 1.8679\n","\n","Epoch 00010: val_loss did not improve from 1.86190\n","Epoch 11/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.8339 - val_loss: 1.8785\n","\n","Epoch 00011: val_loss did not improve from 1.86190\n","Epoch 12/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.8145 - val_loss: 1.8506\n","\n","Epoch 00012: val_loss improved from 1.86190 to 1.85061, saving model to best_model_1.h5\n","Epoch 13/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.8091 - val_loss: 1.8443\n","\n","Epoch 00013: val_loss improved from 1.85061 to 1.84428, saving model to best_model_1.h5\n","Epoch 14/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.8120 - val_loss: 1.8325\n","\n","Epoch 00014: val_loss improved from 1.84428 to 1.83245, saving model to best_model_1.h5\n","Epoch 15/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.8188 - val_loss: 1.8336\n","\n","Epoch 00015: val_loss did not improve from 1.83245\n","Epoch 16/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.7953 - val_loss: 1.8210\n","\n","Epoch 00016: val_loss improved from 1.83245 to 1.82096, saving model to best_model_1.h5\n","Epoch 17/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.8022 - val_loss: 1.8485\n","\n","Epoch 00017: val_loss did not improve from 1.82096\n","Epoch 18/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.7805 - val_loss: 1.8121\n","\n","Epoch 00018: val_loss improved from 1.82096 to 1.81212, saving model to best_model_1.h5\n","Epoch 19/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.7707 - val_loss: 1.8321\n","\n","Epoch 00019: val_loss did not improve from 1.81212\n","Epoch 20/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.7791 - val_loss: 1.8029\n","\n","Epoch 00020: val_loss improved from 1.81212 to 1.80294, saving model to best_model_1.h5\n","Epoch 21/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.7688 - val_loss: 1.8081\n","\n","Epoch 00021: val_loss did not improve from 1.80294\n","Epoch 22/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.7571 - val_loss: 1.7923\n","\n","Epoch 00022: val_loss improved from 1.80294 to 1.79228, saving model to best_model_1.h5\n","Epoch 23/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7506 - val_loss: 1.7871\n","\n","Epoch 00023: val_loss improved from 1.79228 to 1.78710, saving model to best_model_1.h5\n","Epoch 24/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7702 - val_loss: 1.7910\n","\n","Epoch 00024: val_loss did not improve from 1.78710\n","Epoch 25/30\n","326/326 [==============================] - 4s 13ms/step - loss: 1.7501 - val_loss: 1.7728\n","\n","Epoch 00025: val_loss improved from 1.78710 to 1.77278, saving model to best_model_1.h5\n","Epoch 26/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7511 - val_loss: 1.7828\n","\n","Epoch 00026: val_loss did not improve from 1.77278\n","Epoch 27/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7380 - val_loss: 1.7666\n","\n","Epoch 00027: val_loss improved from 1.77278 to 1.76663, saving model to best_model_1.h5\n","Epoch 28/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7302 - val_loss: 1.7551\n","\n","Epoch 00028: val_loss improved from 1.76663 to 1.75509, saving model to best_model_1.h5\n","Epoch 29/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7309 - val_loss: 1.7480\n","\n","Epoch 00029: val_loss improved from 1.75509 to 1.74799, saving model to best_model_1.h5\n","Epoch 30/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.7248 - val_loss: 1.7434\n","\n","Epoch 00030: val_loss improved from 1.74799 to 1.74337, saving model to best_model_1.h5\n","==================== 0.7day2 ====================\n","Epoch 1/30\n","326/326 [==============================] - 6s 15ms/step - loss: 8.0364 - val_loss: 2.1195\n","\n","Epoch 00001: val_loss improved from inf to 2.11953, saving model to best_model_2.h5\n","Epoch 2/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.0798 - val_loss: 2.0461\n","\n","Epoch 00002: val_loss improved from 2.11953 to 2.04612, saving model to best_model_2.h5\n","Epoch 3/30\n","326/326 [==============================] - 5s 14ms/step - loss: 2.0001 - val_loss: 1.9962\n","\n","Epoch 00003: val_loss improved from 2.04612 to 1.99623, saving model to best_model_2.h5\n","Epoch 4/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.9545 - val_loss: 1.9397\n","\n","Epoch 00004: val_loss improved from 1.99623 to 1.93973, saving model to best_model_2.h5\n","Epoch 5/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.9201 - val_loss: 1.9303\n","\n","Epoch 00005: val_loss improved from 1.93973 to 1.93029, saving model to best_model_2.h5\n","Epoch 6/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8834 - val_loss: 1.9132\n","\n","Epoch 00006: val_loss improved from 1.93029 to 1.91323, saving model to best_model_2.h5\n","Epoch 7/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8941 - val_loss: 1.9088\n","\n","Epoch 00007: val_loss improved from 1.91323 to 1.90877, saving model to best_model_2.h5\n","Epoch 8/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8731 - val_loss: 1.8900\n","\n","Epoch 00008: val_loss improved from 1.90877 to 1.88998, saving model to best_model_2.h5\n","Epoch 9/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8712 - val_loss: 1.8835\n","\n","Epoch 00009: val_loss improved from 1.88998 to 1.88352, saving model to best_model_2.h5\n","Epoch 10/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8590 - val_loss: 1.8766\n","\n","Epoch 00010: val_loss improved from 1.88352 to 1.87659, saving model to best_model_2.h5\n","Epoch 11/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8617 - val_loss: 1.8811\n","\n","Epoch 00011: val_loss did not improve from 1.87659\n","Epoch 12/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8452 - val_loss: 1.8673\n","\n","Epoch 00012: val_loss improved from 1.87659 to 1.86730, saving model to best_model_2.h5\n","Epoch 13/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8391 - val_loss: 1.8593\n","\n","Epoch 00013: val_loss improved from 1.86730 to 1.85931, saving model to best_model_2.h5\n","Epoch 14/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8241 - val_loss: 1.8530\n","\n","Epoch 00014: val_loss improved from 1.85931 to 1.85305, saving model to best_model_2.h5\n","Epoch 15/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8374 - val_loss: 1.8727\n","\n","Epoch 00015: val_loss did not improve from 1.85305\n","Epoch 16/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8396 - val_loss: 1.8421\n","\n","Epoch 00016: val_loss improved from 1.85305 to 1.84214, saving model to best_model_2.h5\n","Epoch 17/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8215 - val_loss: 1.8384\n","\n","Epoch 00017: val_loss improved from 1.84214 to 1.83842, saving model to best_model_2.h5\n","Epoch 18/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8040 - val_loss: 1.8623\n","\n","Epoch 00018: val_loss did not improve from 1.83842\n","Epoch 19/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.8222 - val_loss: 1.8232\n","\n","Epoch 00019: val_loss improved from 1.83842 to 1.82325, saving model to best_model_2.h5\n","Epoch 20/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.7971 - val_loss: 1.8581\n","\n","Epoch 00020: val_loss did not improve from 1.82325\n","Epoch 21/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.7877 - val_loss: 1.8148\n","\n","Epoch 00021: val_loss improved from 1.82325 to 1.81479, saving model to best_model_2.h5\n","Epoch 22/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.7937 - val_loss: 1.8079\n","\n","Epoch 00022: val_loss improved from 1.81479 to 1.80788, saving model to best_model_2.h5\n","Epoch 23/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.7770 - val_loss: 1.8088\n","\n","Epoch 00023: val_loss did not improve from 1.80788\n","Epoch 24/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.7806 - val_loss: 1.8038\n","\n","Epoch 00024: val_loss improved from 1.80788 to 1.80376, saving model to best_model_2.h5\n","Epoch 25/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.7628 - val_loss: 1.7931\n","\n","Epoch 00025: val_loss improved from 1.80376 to 1.79311, saving model to best_model_2.h5\n","Epoch 26/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.7650 - val_loss: 1.7839\n","\n","Epoch 00026: val_loss improved from 1.79311 to 1.78392, saving model to best_model_2.h5\n","Epoch 27/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.7506 - val_loss: 1.7782\n","\n","Epoch 00027: val_loss improved from 1.78392 to 1.77815, saving model to best_model_2.h5\n","Epoch 28/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.7444 - val_loss: 1.7703\n","\n","Epoch 00028: val_loss improved from 1.77815 to 1.77027, saving model to best_model_2.h5\n","Epoch 29/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.7361 - val_loss: 1.7591\n","\n","Epoch 00029: val_loss improved from 1.77027 to 1.75909, saving model to best_model_2.h5\n","Epoch 30/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.7492 - val_loss: 1.7649\n","\n","Epoch 00030: val_loss did not improve from 1.75909\n","==================== 0.8day1 ====================\n","Epoch 1/30\n","326/326 [==============================] - 6s 16ms/step - loss: 9.4063 - val_loss: 1.5443\n","\n","Epoch 00001: val_loss improved from inf to 1.54429, saving model to best_model_1.h5\n","Epoch 2/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.4934 - val_loss: 1.4446\n","\n","Epoch 00002: val_loss improved from 1.54429 to 1.44461, saving model to best_model_1.h5\n","Epoch 3/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.4132 - val_loss: 1.4043\n","\n","Epoch 00003: val_loss improved from 1.44461 to 1.40430, saving model to best_model_1.h5\n","Epoch 4/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.3757 - val_loss: 1.4041\n","\n","Epoch 00004: val_loss improved from 1.40430 to 1.40406, saving model to best_model_1.h5\n","Epoch 5/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.3530 - val_loss: 1.3600\n","\n","Epoch 00005: val_loss improved from 1.40406 to 1.35999, saving model to best_model_1.h5\n","Epoch 6/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.3330 - val_loss: 1.3408\n","\n","Epoch 00006: val_loss improved from 1.35999 to 1.34078, saving model to best_model_1.h5\n","Epoch 7/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.3161 - val_loss: 1.3328\n","\n","Epoch 00007: val_loss improved from 1.34078 to 1.33281, saving model to best_model_1.h5\n","Epoch 8/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.3191 - val_loss: 1.3237\n","\n","Epoch 00008: val_loss improved from 1.33281 to 1.32374, saving model to best_model_1.h5\n","Epoch 9/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.3066 - val_loss: 1.3467\n","\n","Epoch 00009: val_loss did not improve from 1.32374\n","Epoch 10/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.3009 - val_loss: 1.3141\n","\n","Epoch 00010: val_loss improved from 1.32374 to 1.31413, saving model to best_model_1.h5\n","Epoch 11/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2953 - val_loss: 1.3182\n","\n","Epoch 00011: val_loss did not improve from 1.31413\n","Epoch 12/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2825 - val_loss: 1.3113\n","\n","Epoch 00012: val_loss improved from 1.31413 to 1.31127, saving model to best_model_1.h5\n","Epoch 13/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2886 - val_loss: 1.3117\n","\n","Epoch 00013: val_loss did not improve from 1.31127\n","Epoch 14/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2744 - val_loss: 1.2991\n","\n","Epoch 00014: val_loss improved from 1.31127 to 1.29909, saving model to best_model_1.h5\n","Epoch 15/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2849 - val_loss: 1.2949\n","\n","Epoch 00015: val_loss improved from 1.29909 to 1.29489, saving model to best_model_1.h5\n","Epoch 16/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2717 - val_loss: 1.2943\n","\n","Epoch 00016: val_loss improved from 1.29489 to 1.29431, saving model to best_model_1.h5\n","Epoch 17/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2776 - val_loss: 1.2899\n","\n","Epoch 00017: val_loss improved from 1.29431 to 1.28992, saving model to best_model_1.h5\n","Epoch 18/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2666 - val_loss: 1.3052\n","\n","Epoch 00018: val_loss did not improve from 1.28992\n","Epoch 19/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2728 - val_loss: 1.2959\n","\n","Epoch 00019: val_loss did not improve from 1.28992\n","Epoch 20/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2627 - val_loss: 1.2842\n","\n","Epoch 00020: val_loss improved from 1.28992 to 1.28418, saving model to best_model_1.h5\n","Epoch 21/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2632 - val_loss: 1.2820\n","\n","Epoch 00021: val_loss improved from 1.28418 to 1.28202, saving model to best_model_1.h5\n","Epoch 22/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2663 - val_loss: 1.2774\n","\n","Epoch 00022: val_loss improved from 1.28202 to 1.27742, saving model to best_model_1.h5\n","Epoch 23/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2610 - val_loss: 1.2888\n","\n","Epoch 00023: val_loss did not improve from 1.27742\n","Epoch 24/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2475 - val_loss: 1.2761\n","\n","Epoch 00024: val_loss improved from 1.27742 to 1.27609, saving model to best_model_1.h5\n","Epoch 25/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2514 - val_loss: 1.2829\n","\n","Epoch 00025: val_loss did not improve from 1.27609\n","Epoch 26/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2508 - val_loss: 1.2663\n","\n","Epoch 00026: val_loss improved from 1.27609 to 1.26634, saving model to best_model_1.h5\n","Epoch 27/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2442 - val_loss: 1.2667\n","\n","Epoch 00027: val_loss did not improve from 1.26634\n","Epoch 28/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2479 - val_loss: 1.2631\n","\n","Epoch 00028: val_loss improved from 1.26634 to 1.26307, saving model to best_model_1.h5\n","Epoch 29/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2461 - val_loss: 1.2609\n","\n","Epoch 00029: val_loss improved from 1.26307 to 1.26092, saving model to best_model_1.h5\n","Epoch 30/30\n","326/326 [==============================] - 5s 15ms/step - loss: 1.2380 - val_loss: 1.2587\n","\n","Epoch 00030: val_loss improved from 1.26092 to 1.25869, saving model to best_model_1.h5\n","==================== 0.8day2 ====================\n","Epoch 1/30\n","326/326 [==============================] - 5s 14ms/step - loss: 8.9907 - val_loss: 1.5939\n","\n","Epoch 00001: val_loss improved from inf to 1.59391, saving model to best_model_2.h5\n","Epoch 2/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.5453 - val_loss: 1.5097\n","\n","Epoch 00002: val_loss improved from 1.59391 to 1.50967, saving model to best_model_2.h5\n","Epoch 3/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.4838 - val_loss: 1.4634\n","\n","Epoch 00003: val_loss improved from 1.50967 to 1.46341, saving model to best_model_2.h5\n","Epoch 4/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.4311 - val_loss: 1.4114\n","\n","Epoch 00004: val_loss improved from 1.46341 to 1.41144, saving model to best_model_2.h5\n","Epoch 5/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.3932 - val_loss: 1.3822\n","\n","Epoch 00005: val_loss improved from 1.41144 to 1.38224, saving model to best_model_2.h5\n","Epoch 6/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3607 - val_loss: 1.3634\n","\n","Epoch 00006: val_loss improved from 1.38224 to 1.36343, saving model to best_model_2.h5\n","Epoch 7/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3439 - val_loss: 1.3548\n","\n","Epoch 00007: val_loss improved from 1.36343 to 1.35479, saving model to best_model_2.h5\n","Epoch 8/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3326 - val_loss: 1.3425\n","\n","Epoch 00008: val_loss improved from 1.35479 to 1.34251, saving model to best_model_2.h5\n","Epoch 9/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3375 - val_loss: 1.3513\n","\n","Epoch 00009: val_loss did not improve from 1.34251\n","Epoch 10/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3171 - val_loss: 1.3294\n","\n","Epoch 00010: val_loss improved from 1.34251 to 1.32935, saving model to best_model_2.h5\n","Epoch 11/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3223 - val_loss: 1.3327\n","\n","Epoch 00011: val_loss did not improve from 1.32935\n","Epoch 12/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3049 - val_loss: 1.3221\n","\n","Epoch 00012: val_loss improved from 1.32935 to 1.32214, saving model to best_model_2.h5\n","Epoch 13/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3107 - val_loss: 1.3155\n","\n","Epoch 00013: val_loss improved from 1.32214 to 1.31554, saving model to best_model_2.h5\n","Epoch 14/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3032 - val_loss: 1.3278\n","\n","Epoch 00014: val_loss did not improve from 1.31554\n","Epoch 15/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3044 - val_loss: 1.3150\n","\n","Epoch 00015: val_loss improved from 1.31554 to 1.31498, saving model to best_model_2.h5\n","Epoch 16/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.3014 - val_loss: 1.3046\n","\n","Epoch 00016: val_loss improved from 1.31498 to 1.30457, saving model to best_model_2.h5\n","Epoch 17/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2864 - val_loss: 1.3029\n","\n","Epoch 00017: val_loss improved from 1.30457 to 1.30294, saving model to best_model_2.h5\n","Epoch 18/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2926 - val_loss: 1.3000\n","\n","Epoch 00018: val_loss improved from 1.30294 to 1.30003, saving model to best_model_2.h5\n","Epoch 19/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2858 - val_loss: 1.3080\n","\n","Epoch 00019: val_loss did not improve from 1.30003\n","Epoch 20/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2752 - val_loss: 1.2963\n","\n","Epoch 00020: val_loss improved from 1.30003 to 1.29629, saving model to best_model_2.h5\n","Epoch 21/30\n","326/326 [==============================] - 5s 14ms/step - loss: 1.2751 - val_loss: 1.2949\n","\n","Epoch 00021: val_loss improved from 1.29629 to 1.29489, saving model to best_model_2.h5\n","Epoch 22/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2758 - val_loss: 1.3140\n","\n","Epoch 00022: val_loss did not improve from 1.29489\n","Epoch 23/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2764 - val_loss: 1.2894\n","\n","Epoch 00023: val_loss improved from 1.29489 to 1.28941, saving model to best_model_2.h5\n","Epoch 24/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2634 - val_loss: 1.2828\n","\n","Epoch 00024: val_loss improved from 1.28941 to 1.28284, saving model to best_model_2.h5\n","Epoch 25/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2684 - val_loss: 1.2770\n","\n","Epoch 00025: val_loss improved from 1.28284 to 1.27703, saving model to best_model_2.h5\n","Epoch 26/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2609 - val_loss: 1.2737\n","\n","Epoch 00026: val_loss improved from 1.27703 to 1.27373, saving model to best_model_2.h5\n","Epoch 27/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2673 - val_loss: 1.2777\n","\n","Epoch 00027: val_loss did not improve from 1.27373\n","Epoch 28/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2606 - val_loss: 1.2675\n","\n","Epoch 00028: val_loss improved from 1.27373 to 1.26749, saving model to best_model_2.h5\n","Epoch 29/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2603 - val_loss: 1.2678\n","\n","Epoch 00029: val_loss did not improve from 1.26749\n","Epoch 30/30\n","326/326 [==============================] - 4s 14ms/step - loss: 1.2471 - val_loss: 1.2625\n","\n","Epoch 00030: val_loss improved from 1.26749 to 1.26248, saving model to best_model_2.h5\n","==================== 0.9day1 ====================\n","Epoch 1/30\n","326/326 [==============================] - 6s 15ms/step - loss: 10.0298 - val_loss: 0.9368\n","\n","Epoch 00001: val_loss improved from inf to 0.93679, saving model to best_model_1.h5\n","Epoch 2/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.8882 - val_loss: 0.8389\n","\n","Epoch 00002: val_loss improved from 0.93679 to 0.83894, saving model to best_model_1.h5\n","Epoch 3/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.8115 - val_loss: 0.7899\n","\n","Epoch 00003: val_loss improved from 0.83894 to 0.78995, saving model to best_model_1.h5\n","Epoch 4/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.7721 - val_loss: 0.7575\n","\n","Epoch 00004: val_loss improved from 0.78995 to 0.75750, saving model to best_model_1.h5\n","Epoch 5/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.7352 - val_loss: 0.7349\n","\n","Epoch 00005: val_loss improved from 0.75750 to 0.73486, saving model to best_model_1.h5\n","Epoch 6/30\n","326/326 [==============================] - 5s 15ms/step - loss: 0.7178 - val_loss: 0.7171\n","\n","Epoch 00006: val_loss improved from 0.73486 to 0.71711, saving model to best_model_1.h5\n","Epoch 7/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.7042 - val_loss: 0.7120\n","\n","Epoch 00007: val_loss improved from 0.71711 to 0.71195, saving model to best_model_1.h5\n","Epoch 8/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6961 - val_loss: 0.7133\n","\n","Epoch 00008: val_loss did not improve from 0.71195\n","Epoch 9/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6947 - val_loss: 0.6999\n","\n","Epoch 00009: val_loss improved from 0.71195 to 0.69987, saving model to best_model_1.h5\n","Epoch 10/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6893 - val_loss: 0.7075\n","\n","Epoch 00010: val_loss did not improve from 0.69987\n","Epoch 11/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6815 - val_loss: 0.6925\n","\n","Epoch 00011: val_loss improved from 0.69987 to 0.69250, saving model to best_model_1.h5\n","Epoch 12/30\n","326/326 [==============================] - 5s 15ms/step - loss: 0.6824 - val_loss: 0.6925\n","\n","Epoch 00012: val_loss did not improve from 0.69250\n","Epoch 13/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6852 - val_loss: 0.6945\n","\n","Epoch 00013: val_loss did not improve from 0.69250\n","Epoch 14/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6772 - val_loss: 0.6934\n","\n","Epoch 00014: val_loss did not improve from 0.69250\n","Epoch 15/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6760 - val_loss: 0.6895\n","\n","Epoch 00015: val_loss improved from 0.69250 to 0.68954, saving model to best_model_1.h5\n","Epoch 16/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6742 - val_loss: 0.6829\n","\n","Epoch 00016: val_loss improved from 0.68954 to 0.68291, saving model to best_model_1.h5\n","Epoch 17/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6743 - val_loss: 0.6823\n","\n","Epoch 00017: val_loss improved from 0.68291 to 0.68226, saving model to best_model_1.h5\n","Epoch 18/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6709 - val_loss: 0.6794\n","\n","Epoch 00018: val_loss improved from 0.68226 to 0.67937, saving model to best_model_1.h5\n","Epoch 19/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6728 - val_loss: 0.6801\n","\n","Epoch 00019: val_loss did not improve from 0.67937\n","Epoch 20/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6737 - val_loss: 0.6770\n","\n","Epoch 00020: val_loss improved from 0.67937 to 0.67699, saving model to best_model_1.h5\n","Epoch 21/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6672 - val_loss: 0.6758\n","\n","Epoch 00021: val_loss improved from 0.67699 to 0.67575, saving model to best_model_1.h5\n","Epoch 22/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6622 - val_loss: 0.6781\n","\n","Epoch 00022: val_loss did not improve from 0.67575\n","Epoch 23/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6600 - val_loss: 0.6765\n","\n","Epoch 00023: val_loss did not improve from 0.67575\n","Epoch 24/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6593 - val_loss: 0.6717\n","\n","Epoch 00024: val_loss improved from 0.67575 to 0.67173, saving model to best_model_1.h5\n","Epoch 25/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6611 - val_loss: 0.6705\n","\n","Epoch 00025: val_loss improved from 0.67173 to 0.67052, saving model to best_model_1.h5\n","Epoch 26/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6605 - val_loss: 0.6711\n","\n","Epoch 00026: val_loss did not improve from 0.67052\n","Epoch 27/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6604 - val_loss: 0.6765\n","\n","Epoch 00027: val_loss did not improve from 0.67052\n","Epoch 28/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6583 - val_loss: 0.6681\n","\n","Epoch 00028: val_loss improved from 0.67052 to 0.66813, saving model to best_model_1.h5\n","Epoch 29/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6556 - val_loss: 0.6743\n","\n","Epoch 00029: val_loss did not improve from 0.66813\n","Epoch 30/30\n","326/326 [==============================] - 5s 14ms/step - loss: 0.6510 - val_loss: 0.6663\n","\n","Epoch 00030: val_loss improved from 0.66813 to 0.66631, saving model to best_model_1.h5\n","==================== 0.9day2 ====================\n","Epoch 1/30\n","326/326 [==============================] - 6s 14ms/step - loss: 9.2165 - val_loss: 0.9543\n","\n","Epoch 00001: val_loss improved from inf to 0.95431, saving model to best_model_2.h5\n","Epoch 2/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.9054 - val_loss: 0.8240\n","\n","Epoch 00002: val_loss improved from 0.95431 to 0.82400, saving model to best_model_2.h5\n","Epoch 3/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.7959 - val_loss: 0.7707\n","\n","Epoch 00003: val_loss improved from 0.82400 to 0.77073, saving model to best_model_2.h5\n","Epoch 4/30\n","326/326 [==============================] - 4s 13ms/step - loss: 0.7542 - val_loss: 0.7498\n","\n","Epoch 00004: val_loss improved from 0.77073 to 0.74982, saving model to best_model_2.h5\n","Epoch 5/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.7424 - val_loss: 0.7549\n","\n","Epoch 00005: val_loss did not improve from 0.74982\n","Epoch 6/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.7265 - val_loss: 0.7368\n","\n","Epoch 00006: val_loss improved from 0.74982 to 0.73682, saving model to best_model_2.h5\n","Epoch 7/30\n","326/326 [==============================] - 4s 13ms/step - loss: 0.7211 - val_loss: 0.7232\n","\n","Epoch 00007: val_loss improved from 0.73682 to 0.72318, saving model to best_model_2.h5\n","Epoch 8/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.7173 - val_loss: 0.7265\n","\n","Epoch 00008: val_loss did not improve from 0.72318\n","Epoch 9/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.7083 - val_loss: 0.7276\n","\n","Epoch 00009: val_loss did not improve from 0.72318\n","Epoch 10/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.7081 - val_loss: 0.7264\n","\n","Epoch 00010: val_loss did not improve from 0.72318\n","Epoch 11/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.7051 - val_loss: 0.7060\n","\n","Epoch 00011: val_loss improved from 0.72318 to 0.70599, saving model to best_model_2.h5\n","Epoch 12/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.7040 - val_loss: 0.7049\n","\n","Epoch 00012: val_loss improved from 0.70599 to 0.70492, saving model to best_model_2.h5\n","Epoch 13/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6990 - val_loss: 0.7203\n","\n","Epoch 00013: val_loss did not improve from 0.70492\n","Epoch 14/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6936 - val_loss: 0.6980\n","\n","Epoch 00014: val_loss improved from 0.70492 to 0.69805, saving model to best_model_2.h5\n","Epoch 15/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6861 - val_loss: 0.6959\n","\n","Epoch 00015: val_loss improved from 0.69805 to 0.69593, saving model to best_model_2.h5\n","Epoch 16/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6901 - val_loss: 0.6953\n","\n","Epoch 00016: val_loss improved from 0.69593 to 0.69533, saving model to best_model_2.h5\n","Epoch 17/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6921 - val_loss: 0.6917\n","\n","Epoch 00017: val_loss improved from 0.69533 to 0.69174, saving model to best_model_2.h5\n","Epoch 18/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6856 - val_loss: 0.6937\n","\n","Epoch 00018: val_loss did not improve from 0.69174\n","Epoch 19/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6936 - val_loss: 0.6915\n","\n","Epoch 00019: val_loss improved from 0.69174 to 0.69149, saving model to best_model_2.h5\n","Epoch 20/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6862 - val_loss: 0.6866\n","\n","Epoch 00020: val_loss improved from 0.69149 to 0.68659, saving model to best_model_2.h5\n","Epoch 21/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6750 - val_loss: 0.6876\n","\n","Epoch 00021: val_loss did not improve from 0.68659\n","Epoch 22/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6768 - val_loss: 0.6830\n","\n","Epoch 00022: val_loss improved from 0.68659 to 0.68303, saving model to best_model_2.h5\n","Epoch 23/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6778 - val_loss: 0.6822\n","\n","Epoch 00023: val_loss improved from 0.68303 to 0.68222, saving model to best_model_2.h5\n","Epoch 24/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6716 - val_loss: 0.6797\n","\n","Epoch 00024: val_loss improved from 0.68222 to 0.67971, saving model to best_model_2.h5\n","Epoch 25/30\n","326/326 [==============================] - 4s 14ms/step - loss: 0.6741 - val_loss: 0.6792\n","\n","Epoch 00025: val_loss improved from 0.67971 to 0.67916, saving model to best_model_2.h5\n","Epoch 26/30\n","326/326 [==============================] - 4s 13ms/step - loss: 0.6724 - val_loss: 0.6791\n","\n","Epoch 00026: val_loss improved from 0.67916 to 0.67914, saving model to best_model_2.h5\n","Epoch 27/30\n","326/326 [==============================] - 4s 13ms/step - loss: 0.6650 - val_loss: 0.6834\n","\n","Epoch 00027: val_loss did not improve from 0.67914\n","Epoch 28/30\n","326/326 [==============================] - 4s 13ms/step - loss: 0.6725 - val_loss: 0.6745\n","\n","Epoch 00028: val_loss improved from 0.67914 to 0.67454, saving model to best_model_2.h5\n","Epoch 29/30\n","326/326 [==============================] - 4s 13ms/step - loss: 0.6669 - val_loss: 0.6765\n","\n","Epoch 00029: val_loss did not improve from 0.67454\n","Epoch 30/30\n","326/326 [==============================] - 4s 13ms/step - loss: 0.6664 - val_loss: 0.6794\n","\n","Epoch 00030: val_loss did not improve from 0.67454\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kKU5Wvs2_4Z7"},"source":["# Prediction 및 Submission 파일 생성\n","df_final=pd.DataFrame(np.array(prediction).T)\n","submission=pd.read_csv('/content/drive/MyDrive/dacon/solar/sample_submission.csv')\n","cols=submission.columns.tolist()[1:]\n","submission[cols]=df_final.values\n","submission.to_csv('/content/drive/MyDrive/dacon/solar/submission.csv',index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MgQT6-POJLBN"},"source":[""],"execution_count":null,"outputs":[]}]}